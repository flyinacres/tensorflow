{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.tensorflow.org/text/guide/subwords_tokenizer","metadata":{}},{"cell_type":"code","source":"pip install -q -U \"tensorflow-text==2.11.*\"","metadata":{"execution":{"iopub.status.busy":"2024-06-04T16:51:35.460299Z","iopub.execute_input":"2024-06-04T16:51:35.460760Z","iopub.status.idle":"2024-06-04T16:53:16.199173Z","shell.execute_reply.started":"2024-06-04T16:51:35.460723Z","shell.execute_reply":"2024-06-04T16:53:16.197717Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.3.1 requires aiohttp!=4.0.0a0,!=4.0.0a1, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nonnx 1.16.1 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\ntensorboardx 2.6.2.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.11.1 which is incompatible.\ntensorflow-serving-api 2.14.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-serving-api 2.14.1 requires tensorflow<3,>=2.14.1, but you have tensorflow 2.11.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -q tensorflow_datasets","metadata":{"execution":{"iopub.status.busy":"2024-06-04T16:53:49.667262Z","iopub.execute_input":"2024-06-04T16:53:49.667782Z","iopub.status.idle":"2024-06-04T16:54:04.917434Z","shell.execute_reply.started":"2024-06-04T16:53:49.667730Z","shell.execute_reply":"2024-06-04T16:54:04.916070Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import collections\nimport os\nimport pathlib\nimport re\nimport string\nimport sys\nimport tempfile\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow_datasets as tfds\nimport tensorflow_text as text\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-06-04T16:54:04.919369Z","iopub.execute_input":"2024-06-04T16:54:04.919866Z","iopub.status.idle":"2024-06-04T16:54:13.169719Z","shell.execute_reply.started":"2024-06-04T16:54:04.919819Z","shell.execute_reply":"2024-06-04T16:54:13.168161Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tf.get_logger().setLevel('ERROR')\npwd = pathlib.Path.cwd()","metadata":{"execution":{"iopub.status.busy":"2024-06-04T17:10:14.974613Z","iopub.execute_input":"2024-06-04T17:10:14.976168Z","iopub.status.idle":"2024-06-04T17:10:14.987897Z","shell.execute_reply.started":"2024-06-04T17:10:14.976109Z","shell.execute_reply":"2024-06-04T17:10:14.986394Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n                               as_supervised=True)\ntrain_examples, val_examples = examples['train'], examples['validation']","metadata":{"execution":{"iopub.status.busy":"2024-06-04T17:10:19.618721Z","iopub.execute_input":"2024-06-04T17:10:19.619175Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset 124.94 MiB (download: 124.94 MiB, generated: Unknown size, total: 124.94 MiB) to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f950c0d4228344e5be671f8f8ab79b14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"333036c23d7242cca6f657fbffa5755a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e3abbc8d6f3476d90c54d1aca615314"}},"metadata":{}}]},{"cell_type":"code","source":"for pt, en in train_examples.take(1):\n  print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n  print(\"English:   \", en.numpy().decode('utf-8'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_en = train_examples.map(lambda pt, en: en)\ntrain_pt = train_examples.map(lambda pt, en: pt)","metadata":{},"execution_count":null,"outputs":[]}]}