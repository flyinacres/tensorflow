{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gym[classic_control]\n!pip install pyglet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-15T18:23:54.500943Z","iopub.execute_input":"2024-06-15T18:23:54.501387Z","iopub.status.idle":"2024-06-15T18:24:29.278261Z","shell.execute_reply.started":"2024-06-15T18:23:54.501355Z","shell.execute_reply":"2024-06-15T18:24:29.276776Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gym[classic_control] in /opt/conda/lib/python3.10/site-packages (0.26.2)\n\u001b[33mWARNING: gym 0.26.2 does not provide the extra 'classic-control'\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym[classic_control]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym[classic_control]) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym[classic_control]) (0.0.8)\nCollecting pygame==2.1.0 (from gym[classic_control])\n  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\nDownloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pygame\nSuccessfully installed pygame-2.1.0\nCollecting pyglet\n  Downloading pyglet-2.0.15-py3-none-any.whl.metadata (7.8 kB)\nDownloading pyglet-2.0.15-py3-none-any.whl (884 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.3/884.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyglet\nSuccessfully installed pyglet-2.0.15\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install additional packages for visualization\n!sudo apt-get install -y python-opengl > /dev/null 2>&1\n!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:25:07.260084Z","iopub.execute_input":"2024-06-15T18:25:07.260576Z","iopub.status.idle":"2024-06-15T18:25:31.041556Z","shell.execute_reply.started":"2024-06-15T18:25:07.260523Z","shell.execute_reply":"2024-06-15T18:25:31.039963Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import collections\nimport gym\nimport numpy as np\nimport statistics\nimport tensorflow as tf\nimport tqdm\n\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers\nfrom typing import Any, List, Sequence, Tuple\n\n\n# Create the environment\nenv = gym.make(\"CartPole-v1\")\n\n# Set seed for experiment reproducibility\nseed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\n# Small epsilon value for stabilizing division operations\neps = np.finfo(np.float32).eps.item()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:25:31.044452Z","iopub.execute_input":"2024-06-15T18:25:31.044816Z","iopub.status.idle":"2024-06-15T18:25:46.867422Z","shell.execute_reply.started":"2024-06-15T18:25:31.044777Z","shell.execute_reply":"2024-06-15T18:25:46.866161Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-06-15 18:25:33.743706: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-15 18:25:33.743870: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-15 18:25:33.914587: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class ActorCritic(tf.keras.Model):\n  \"\"\"Combined actor-critic network.\"\"\"\n\n  def __init__(\n      self,\n      num_actions: int,\n      num_hidden_units: int):\n    \"\"\"Initialize.\"\"\"\n    super().__init__()\n\n    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n    self.actor = layers.Dense(num_actions)\n    self.critic = layers.Dense(1)\n\n  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n    x = self.common(inputs)\n    return self.actor(x), self.critic(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:28:25.800478Z","iopub.execute_input":"2024-06-15T18:28:25.802163Z","iopub.status.idle":"2024-06-15T18:28:25.811241Z","shell.execute_reply.started":"2024-06-15T18:28:25.802116Z","shell.execute_reply":"2024-06-15T18:28:25.809852Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"num_actions = env.action_space.n  # 2\nnum_hidden_units = 128\n\nmodel = ActorCritic(num_actions, num_hidden_units)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:29:25.337830Z","iopub.execute_input":"2024-06-15T18:29:25.339142Z","iopub.status.idle":"2024-06-15T18:29:25.354642Z","shell.execute_reply.started":"2024-06-15T18:29:25.339092Z","shell.execute_reply":"2024-06-15T18:29:25.353181Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Wrap Gym's `env.step` call as an operation in a TensorFlow function.\n# This would allow it to be included in a callable TensorFlow graph.\n\n@tf.numpy_function(Tout=[tf.float32, tf.int32, tf.int32])\ndef env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n\n  state, reward, done, truncated, info = env.step(action)\n  return (state.astype(np.float32),\n          np.array(reward, np.int32),\n          np.array(done, np.int32))","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:30:51.023270Z","iopub.execute_input":"2024-06-15T18:30:51.023720Z","iopub.status.idle":"2024-06-15T18:30:51.032240Z","shell.execute_reply.started":"2024-06-15T18:30:51.023687Z","shell.execute_reply":"2024-06-15T18:30:51.030769Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def run_episode(\n    initial_state: tf.Tensor,\n    model: tf.keras.Model,\n    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n  \"\"\"Runs a single episode to collect training data.\"\"\"\n\n  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n\n  initial_state_shape = initial_state.shape\n  state = initial_state\n\n  for t in tf.range(max_steps):\n    # Convert state into a batched tensor (batch size = 1)\n    state = tf.expand_dims(state, 0)\n\n    # Run the model and to get action probabilities and critic value\n    action_logits_t, value = model(state)\n\n    # Sample next action from the action probability distribution\n    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n    action_probs_t = tf.nn.softmax(action_logits_t)\n\n    # Store critic values\n    values = values.write(t, tf.squeeze(value))\n\n    # Store log probability of the action chosen\n    action_probs = action_probs.write(t, action_probs_t[0, action])\n\n    # Apply action to the environment to get next state and reward\n    state, reward, done = env_step(action)\n    state.set_shape(initial_state_shape)\n\n    # Store reward\n    rewards = rewards.write(t, reward)\n\n    if tf.cast(done, tf.bool):\n      break\n\n  action_probs = action_probs.stack()\n  values = values.stack()\n  rewards = rewards.stack()\n\n  return action_probs, values, rewards","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:32:27.487730Z","iopub.execute_input":"2024-06-15T18:32:27.488878Z","iopub.status.idle":"2024-06-15T18:32:27.506568Z","shell.execute_reply.started":"2024-06-15T18:32:27.488830Z","shell.execute_reply":"2024-06-15T18:32:27.505101Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_expected_return(\n    rewards: tf.Tensor,\n    gamma: float,\n    standardize: bool = True) -> tf.Tensor:\n  \"\"\"Compute expected returns per timestep.\"\"\"\n\n  n = tf.shape(rewards)[0]\n  returns = tf.TensorArray(dtype=tf.float32, size=n)\n\n  # Start from the end of `rewards` and accumulate reward sums\n  # into the `returns` array\n  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n  discounted_sum = tf.constant(0.0)\n  discounted_sum_shape = discounted_sum.shape\n  for i in tf.range(n):\n    reward = rewards[i]\n    discounted_sum = reward + gamma * discounted_sum\n    discounted_sum.set_shape(discounted_sum_shape)\n    returns = returns.write(i, discounted_sum)\n  returns = returns.stack()[::-1]\n\n  if standardize:\n    returns = ((returns - tf.math.reduce_mean(returns)) /\n               (tf.math.reduce_std(returns) + eps))\n\n  return returns","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:32:42.689196Z","iopub.execute_input":"2024-06-15T18:32:42.689680Z","iopub.status.idle":"2024-06-15T18:32:42.701303Z","shell.execute_reply.started":"2024-06-15T18:32:42.689644Z","shell.execute_reply":"2024-06-15T18:32:42.699206Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n\ndef compute_loss(\n    action_probs: tf.Tensor,\n    values: tf.Tensor,\n    returns: tf.Tensor) -> tf.Tensor:\n  \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n\n  advantage = returns - values\n\n  action_log_probs = tf.math.log(action_probs)\n  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n\n  critic_loss = huber_loss(values, returns)\n\n  return actor_loss + critic_loss","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:32:59.526432Z","iopub.execute_input":"2024-06-15T18:32:59.526865Z","iopub.status.idle":"2024-06-15T18:32:59.534509Z","shell.execute_reply.started":"2024-06-15T18:32:59.526836Z","shell.execute_reply":"2024-06-15T18:32:59.533170Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n\n@tf.function\ndef train_step(\n    initial_state: tf.Tensor,\n    model: tf.keras.Model,\n    optimizer: tf.keras.optimizers.Optimizer,\n    gamma: float,\n    max_steps_per_episode: int) -> tf.Tensor:\n  \"\"\"Runs a model training step.\"\"\"\n\n  with tf.GradientTape() as tape:\n\n    # Run the model for one episode to collect training data\n    action_probs, values, rewards = run_episode(\n        initial_state, model, max_steps_per_episode)\n\n    # Calculate the expected returns\n    returns = get_expected_return(rewards, gamma)\n\n    # Convert training data to appropriate TF tensor shapes\n    action_probs, values, returns = [\n        tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n\n    # Calculate the loss values to update our network\n    loss = compute_loss(action_probs, values, returns)\n\n  # Compute the gradients from the loss\n  grads = tape.gradient(loss, model.trainable_variables)\n\n  # Apply the gradients to the model's parameters\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n  episode_reward = tf.math.reduce_sum(rewards)\n\n  return episode_reward","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:38:29.781846Z","iopub.execute_input":"2024-06-15T18:38:29.782377Z","iopub.status.idle":"2024-06-15T18:38:29.830945Z","shell.execute_reply.started":"2024-06-15T18:38:29.782342Z","shell.execute_reply":"2024-06-15T18:38:29.829609Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmin_episodes_criterion = 100\nmax_episodes = 10000\nmax_steps_per_episode = 500\n\n# `CartPole-v1` is considered solved if average reward is >= 475 over 500\n# consecutive trials\nreward_threshold = 475\nrunning_reward = 0\n\n# The discount factor for future rewards\ngamma = 0.99\n\n# Keep the last episodes reward\nepisodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n\nt = tqdm.trange(max_episodes)\nfor i in t:\n    initial_state, info = env.reset()\n    initial_state = tf.constant(initial_state, dtype=tf.float32)\n    episode_reward = int(train_step(\n        initial_state, model, optimizer, gamma, max_steps_per_episode))\n\n    episodes_reward.append(episode_reward)\n    running_reward = statistics.mean(episodes_reward)\n\n\n    t.set_postfix(\n        episode_reward=episode_reward, running_reward=running_reward)\n\n    # Show the average episode reward every 10 episodes\n    if i % 10 == 0:\n      pass # print(f'Episode {i}: average reward: {avg_reward}')\n\n    if running_reward > reward_threshold and i >= min_episodes_criterion:\n        break\n\nprint(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:39:30.951579Z","iopub.execute_input":"2024-06-15T18:39:30.952074Z","iopub.status.idle":"2024-06-15T18:40:21.209377Z","shell.execute_reply.started":"2024-06-15T18:39:30.952039Z","shell.execute_reply":"2024-06-15T18:40:21.208122Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"  0%|          | 0/10000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\n  3%|▎         | 340/10000 [00:50<23:47,  6.77it/s, episode_reward=500, running_reward=476]","output_type":"stream"},{"name":"stdout","text":"\nSolved at episode 340: average reward: 475.64!\nCPU times: user 1min 23s, sys: 8.89 s, total: 1min 32s\nWall time: 50.2 s\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}