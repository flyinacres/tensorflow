{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"modelInstanceVersion","sourceId":40168,"databundleVersionId":8367325,"modelInstanceId":4723}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://keras.io/guides/keras_nlp/getting_started/","metadata":{}},{"cell_type":"markdown","source":"This is running very slowly without an accelerator--retry with one of them enabled!","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade keras-nlp\n!pip install -q --upgrade keras  # Upgrade to Keras 3.","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:55:33.183916Z","iopub.execute_input":"2024-06-16T15:55:33.184343Z","iopub.status.idle":"2024-06-16T15:56:00.792558Z","shell.execute_reply.started":"2024-06-16T15:55:33.184314Z","shell.execute_reply":"2024-06-16T15:56:00.791337Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n\nimport keras_nlp\nimport keras\n\n# Use mixed precision to speed up all training in this guide.\nkeras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:56:13.028440Z","iopub.execute_input":"2024-06-16T15:56:13.029161Z","iopub.status.idle":"2024-06-16T15:56:13.034929Z","shell.execute_reply.started":"2024-06-16T15:56:13.029126Z","shell.execute_reply":"2024-06-16T15:56:13.033688Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz\n!# Remove unsupervised examples\n!rm -r aclImdb/train/unsup","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:56:15.754689Z","iopub.execute_input":"2024-06-16T15:56:15.755107Z","iopub.status.idle":"2024-06-16T15:56:44.292204Z","shell.execute_reply.started":"2024-06-16T15:56:15.755076Z","shell.execute_reply":"2024-06-16T15:56:44.290689Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  4830k      0  0:00:17  0:00:17 --:--:-- 10.8M\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 16\nimdb_train = keras.utils.text_dataset_from_directory(\n    \"aclImdb/train\",\n    batch_size=BATCH_SIZE,\n)\nimdb_test = keras.utils.text_dataset_from_directory(\n    \"aclImdb/test\",\n    batch_size=BATCH_SIZE,\n)\n\n# Inspect first review\n# Format is (review text tensor, label tensor)\nprint(imdb_train.unbatch().take(1).get_single_element())","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:56:44.294634Z","iopub.execute_input":"2024-06-16T15:56:44.295019Z","iopub.status.idle":"2024-06-16T15:56:47.631090Z","shell.execute_reply.started":"2024-06-16T15:56:44.294983Z","shell.execute_reply":"2024-06-16T15:56:47.629613Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Found 25000 files belonging to 2 classes.\nFound 25000 files belonging to 2 classes.\n(<tf.Tensor: shape=(), dtype=string, numpy=b\"After reading several good reviews as well as hearing nice things about it by word of mouth I decided to rent Come Undone. I must say I was rather disappointed. The story was hard to follow because the film is set as a series of flashbacks between the present and recent past that are very poorly executed. The characters, despite the actors best efforts are flat and uninteresting. The sex is and nudity are more explicit than they need to be. I've never seen a film where they seemed so unnecessary to the plot. The ending is very anti-climatic and leaves many unanswered questions to a story line that wasn't explained well to begin with. In my opinion, a waste of time.\">, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n# Note: batched inputs expected so must wrap string in iterable\nclassifier.predict([\"I love modular workflows in keras-nlp!\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:57:09.387359Z","iopub.execute_input":"2024-06-16T15:57:09.387809Z","iopub.status.idle":"2024-06-16T15:57:28.429022Z","shell.execute_reply.started":"2024-06-16T15:57:09.387777Z","shell.execute_reply":"2024-06-16T15:57:28.427871Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 84 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 0 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\nAttaching 'model.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([[-1.287,  1.393]], dtype=float16)"},"metadata":{}}]},{"cell_type":"code","source":"classifier.evaluate(imdb_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:57:46.126081Z","iopub.execute_input":"2024-06-16T15:57:46.126475Z","iopub.status.idle":"2024-06-16T16:09:27.945625Z","shell.execute_reply.started":"2024-06-16T15:57:46.126443Z","shell.execute_reply":"2024-06-16T16:09:27.944436Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 447ms/step - loss: 0.4592 - sparse_categorical_accuracy: 0.7888\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[0.4550009071826935, 0.7903599739074707]"},"metadata":{}}]},{"cell_type":"code","source":"classifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_tiny_en_uncased\",\n    num_classes=2,\n)\nclassifier.fit(\n    imdb_train,\n    validation_data=imdb_test,\n    epochs=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:26:33.808276Z","iopub.execute_input":"2024-06-16T16:26:33.808752Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1478/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2:00\u001b[0m 1s/step - loss: 0.5192 - sparse_categorical_accuracy: 0.7275","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\n\npreprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n    \"bert_tiny_en_uncased\",\n    sequence_length=512,\n)\n\n# Apply the preprocessor to every sample of train and test data using `map()`.\n# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n# https://www.tensorflow.org/guide/data_performance for details.\n\n# Note: only call `cache()` if you training data fits in CPU memory!\nimdb_train_cached = (\n    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\nimdb_test_cached = (\n    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\n\nclassifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_tiny_en_uncased\", preprocessor=None, num_classes=2\n)\nclassifier.fit(\n    imdb_train_cached,\n    validation_data=imdb_test_cached,\n    epochs=3,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_tiny_en_uncased\")\ntokenizer([\"I love modular workflows!\", \"Libraries over frameworks!\"])\n\n# Write your own packer or use one of our `Layers`\npacker = keras_nlp.layers.MultiSegmentPacker(\n    start_value=tokenizer.cls_token_id,\n    end_value=tokenizer.sep_token_id,\n    # Note: This cannot be longer than the preset's `sequence_length`, and there\n    # is no check for a custom preprocessor!\n    sequence_length=64,\n)\n\n\n# This function that takes a text sample `x` and its\n# corresponding label `y` as input and converts the\n# text into a format suitable for input into a BERT model.\ndef preprocessor(x, y):\n    token_ids, segment_ids = packer(tokenizer(x))\n    x = {\n        \"token_ids\": token_ids,\n        \"segment_ids\": segment_ids,\n        \"padding_mask\": token_ids != 0,\n    }\n    return x, y\n\n\nimdb_train_preprocessed = imdb_train.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n    tf.data.AUTOTUNE\n)\nimdb_test_preprocessed = imdb_test.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n    tf.data.AUTOTUNE\n)\n\n# Preprocessed example\nprint(imdb_train_preprocessed.unbatch().take(1).get_single_element())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\nbackbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")\n\nimdb_train_preprocessed = (\n    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\nimdb_test_preprocessed = (\n    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\n\nbackbone.trainable = False\ninputs = backbone.input\nsequence = backbone(inputs)[\"sequence_output\"]\nfor _ in range(2):\n    sequence = keras_nlp.layers.TransformerEncoder(\n        num_heads=2,\n        intermediate_dim=512,\n        dropout=0.1,\n    )(sequence)\n# Use [CLS] token output to classify\noutputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])\n\nmodel = keras.Model(inputs, outputs)\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.AdamW(5e-5),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    jit_compile=True,\n)\nmodel.summary()\nmodel.fit(\n    imdb_train_preprocessed,\n    validation_data=imdb_test_preprocessed,\n    epochs=3,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All BERT `en` models have the same vocabulary, so reuse preprocessor from\n# \"bert_tiny_en_uncased\"\npreprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n    \"bert_tiny_en_uncased\",\n    sequence_length=256,\n)\npacker = preprocessor.packer\ntokenizer = preprocessor.tokenizer\n\n# keras.Layer to replace some input tokens with the \"[MASK]\" token\nmasker = keras_nlp.layers.MaskedLMMaskGenerator(\n    vocabulary_size=tokenizer.vocabulary_size(),\n    mask_selection_rate=0.25,\n    mask_selection_length=64,\n    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n    unselectable_token_ids=[\n        tokenizer.token_to_id(x) for x in [\"[CLS]\", \"[PAD]\", \"[SEP]\"]\n    ],\n)\n\n\ndef preprocess(inputs, label):\n    inputs = preprocessor(inputs)\n    masked_inputs = masker(inputs[\"token_ids\"])\n    # Split the masking layer outputs into a (features, labels, and weights)\n    # tuple that we can use with keras.Model.fit().\n    features = {\n        \"token_ids\": masked_inputs[\"token_ids\"],\n        \"segment_ids\": inputs[\"segment_ids\"],\n        \"padding_mask\": inputs[\"padding_mask\"],\n        \"mask_positions\": masked_inputs[\"mask_positions\"],\n    }\n    labels = masked_inputs[\"mask_ids\"]\n    weights = masked_inputs[\"mask_weights\"]\n    return features, labels, weights\n\n\npretrain_ds = imdb_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n    tf.data.AUTOTUNE\n)\npretrain_val_ds = imdb_test.map(\n    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n).prefetch(tf.data.AUTOTUNE)\n\n# Tokens with ID 103 are \"masked\"\nprint(pretrain_ds.unbatch().take(1).get_single_element())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BERT backbone\nbackbone = keras_nlp.models.BertBackbone(\n    vocabulary_size=tokenizer.vocabulary_size(),\n    num_layers=2,\n    num_heads=2,\n    hidden_dim=128,\n    intermediate_dim=512,\n)\n\n# Language modeling head\nmlm_head = keras_nlp.layers.MaskedLMHead(\n    token_embedding=backbone.token_embedding,\n)\n\ninputs = {\n    \"token_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"token_ids\"),\n    \"segment_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"segment_ids\"),\n    \"padding_mask\": keras.Input(shape=(None,), dtype=tf.int32, name=\"padding_mask\"),\n    \"mask_positions\": keras.Input(shape=(None,), dtype=tf.int32, name=\"mask_positions\"),\n}\n\n# Encoded token sequence\nsequence = backbone(inputs)[\"sequence_output\"]\n\n# Predict an output word for each masked input token.\n# We use the input token embedding to project from our encoded vectors to\n# vocabulary logits, which has been shown to improve training efficiency.\noutputs = mlm_head(sequence, mask_positions=inputs[\"mask_positions\"])\n\n# Define and compile our pretraining model.\npretraining_model = keras.Model(inputs, outputs)\npretraining_model.summary()\npretraining_model.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.AdamW(learning_rate=5e-4),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    jit_compile=True,\n)\n\n# Pretrain on IMDB dataset\npretraining_model.fit(\n    pretrain_ds,\n    validation_data=pretrain_val_ds,\n    epochs=3,  # Increase to 6 for higher accuracy\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n    imdb_train.map(lambda x, y: x),\n    vocabulary_size=20_000,\n    lowercase=True,\n    strip_accents=True,\n    reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[MASK]\", \"[UNK]\"],\n)\ntokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n    vocabulary=vocab,\n    lowercase=True,\n    strip_accents=True,\n    oov_token=\"[UNK]\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"packer = keras_nlp.layers.StartEndPacker(\n    start_value=tokenizer.token_to_id(\"[START]\"),\n    end_value=tokenizer.token_to_id(\"[END]\"),\n    pad_value=tokenizer.token_to_id(\"[PAD]\"),\n    sequence_length=512,\n)\n\n\ndef preprocess(x, y):\n    token_ids = packer(tokenizer(x))\n    return token_ids, y\n\n\nimdb_preproc_train_ds = imdb_train.map(\n    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n).prefetch(tf.data.AUTOTUNE)\nimdb_preproc_val_ds = imdb_test.map(\n    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n).prefetch(tf.data.AUTOTUNE)\n\nprint(imdb_preproc_train_ds.unbatch().take(1).get_single_element())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_id_input = keras.Input(\n    shape=(None,),\n    dtype=\"int32\",\n    name=\"token_ids\",\n)\noutputs = keras_nlp.layers.TokenAndPositionEmbedding(\n    vocabulary_size=len(vocab),\n    sequence_length=packer.sequence_length,\n    embedding_dim=64,\n)(token_id_input)\noutputs = keras_nlp.layers.TransformerEncoder(\n    num_heads=2,\n    intermediate_dim=128,\n    dropout=0.1,\n)(outputs)\n# Use \"[START]\" token to classify\noutputs = keras.layers.Dense(2)(outputs[:, 0, :])\nmodel = keras.Model(\n    inputs=token_id_input,\n    outputs=outputs,\n)\n\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.AdamW(5e-5),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    jit_compile=True,\n)\nmodel.fit(\n    imdb_preproc_train_ds,\n    validation_data=imdb_preproc_val_ds,\n    epochs=3,\n)","metadata":{},"execution_count":null,"outputs":[]}]}