{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://keras.io/guides/keras_nlp/getting_started/","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade keras-nlp\n!pip install -q --upgrade keras  # Upgrade to Keras 3.","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:44:37.544413Z","iopub.execute_input":"2024-06-16T15:44:37.545705Z","iopub.status.idle":"2024-06-16T15:45:09.058434Z","shell.execute_reply.started":"2024-06-16T15:44:37.545654Z","shell.execute_reply":"2024-06-16T15:45:09.056765Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n\nimport keras_nlp\nimport keras\n\n# Use mixed precision to speed up all training in this guide.\nkeras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:45:37.633087Z","iopub.execute_input":"2024-06-16T15:45:37.633548Z","iopub.status.idle":"2024-06-16T15:45:52.958061Z","shell.execute_reply.started":"2024-06-16T15:45:37.633509Z","shell.execute_reply":"2024-06-16T15:45:52.956828Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-16 15:45:39.833868: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-16 15:45:39.833987: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-16 15:45:39.993756: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz\n!# Remove unsupervised examples\n!rm -r aclImdb/train/unsup","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:45:52.959857Z","iopub.execute_input":"2024-06-16T15:45:52.960473Z","iopub.status.idle":"2024-06-16T15:46:18.779658Z","shell.execute_reply.started":"2024-06-16T15:45:52.960441Z","shell.execute_reply":"2024-06-16T15:46:18.778295Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  5198k      0  0:00:15  0:00:15 --:--:-- 11.3M\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 16\nimdb_train = keras.utils.text_dataset_from_directory(\n    \"aclImdb/train\",\n    batch_size=BATCH_SIZE,\n)\nimdb_test = keras.utils.text_dataset_from_directory(\n    \"aclImdb/test\",\n    batch_size=BATCH_SIZE,\n)\n\n# Inspect first review\n# Format is (review text tensor, label tensor)\nprint(imdb_train.unbatch().take(1).get_single_element())","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:49:45.629819Z","iopub.execute_input":"2024-06-16T15:49:45.630944Z","iopub.status.idle":"2024-06-16T15:49:49.081496Z","shell.execute_reply.started":"2024-06-16T15:49:45.630905Z","shell.execute_reply":"2024-06-16T15:49:49.080322Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found 25000 files belonging to 2 classes.\nFound 25000 files belonging to 2 classes.\n(<tf.Tensor: shape=(), dtype=string, numpy=b'I saw this feature as part of the Asian American Film Festival in New York and was horrified by the graphic, sado-masochistic, child pornography that I witnessed. The story line is hidden beneath way too many graphic sex scenes - and, not one is in the least bit erotic - sick is the more the feeling. The director seemed to be going for shock value rather the exploring the various levels of why these characters are like this. See it if you can stomach it - I still have flashbacks.'>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n# Note: batched inputs expected so must wrap string in iterable\nclassifier.predict([\"I love modular workflows in keras-nlp!\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:51:06.052298Z","iopub.execute_input":"2024-06-16T15:51:06.052748Z","iopub.status.idle":"2024-06-16T15:51:40.259744Z","shell.execute_reply.started":"2024-06-16T15:51:06.052715Z","shell.execute_reply":"2024-06-16T15:51:40.258656Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 84 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 0 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\nAttaching 'model.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array([[-1.287,  1.393]], dtype=float16)"},"metadata":{}}]},{"cell_type":"code","source":"classifier.evaluate(imdb_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_tiny_en_uncased\",\n    num_classes=2,\n)\nclassifier.fit(\n    imdb_train,\n    validation_data=imdb_test,\n    epochs=1,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\npreprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n    \"bert_tiny_en_uncased\",\n    sequence_length=512,\n)\n\n# Apply the preprocessor to every sample of train and test data using `map()`.\n# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n# https://www.tensorflow.org/guide/data_performance for details.\n\n# Note: only call `cache()` if you training data fits in CPU memory!\nimdb_train_cached = (\n    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\nimdb_test_cached = (\n    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\n\nclassifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_tiny_en_uncased\", preprocessor=None, num_classes=2\n)\nclassifier.fit(\n    imdb_train_cached,\n    validation_data=imdb_test_cached,\n    epochs=3,\n)","metadata":{},"execution_count":null,"outputs":[]}]}