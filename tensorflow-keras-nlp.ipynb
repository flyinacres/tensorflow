{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"modelInstanceVersion","sourceId":6054,"databundleVersionId":7429172,"modelInstanceId":4675},{"sourceType":"modelInstanceVersion","sourceId":40168,"databundleVersionId":8367325,"modelInstanceId":4723}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://keras.io/guides/keras_nlp/getting_started/","metadata":{}},{"cell_type":"markdown","source":"This is running very slowly without an accelerator--retry with one of them enabled!","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade keras-nlp\n!pip install -q --upgrade keras  # Upgrade to Keras 3.","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:02:27.714585Z","iopub.execute_input":"2024-06-17T18:02:27.714882Z","iopub.status.idle":"2024-06-17T18:02:58.660598Z","shell.execute_reply.started":"2024-06-17T18:02:27.714856Z","shell.execute_reply":"2024-06-17T18:02:58.659607Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n\nimport keras_nlp\nimport keras\n\n# Use mixed precision to speed up all training in this guide.\nkeras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:02:58.662852Z","iopub.execute_input":"2024-06-17T18:02:58.663213Z","iopub.status.idle":"2024-06-17T18:03:11.403741Z","shell.execute_reply.started":"2024-06-17T18:02:58.663178Z","shell.execute_reply":"2024-06-17T18:03:11.402846Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-17 18:03:00.267041: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-17 18:03:00.267164: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-17 18:03:00.370605: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz\n!# Remove unsupervised examples\n!rm -r aclImdb/train/unsup","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:03:11.405402Z","iopub.execute_input":"2024-06-17T18:03:11.406360Z","iopub.status.idle":"2024-06-17T18:03:25.187150Z","shell.execute_reply.started":"2024-06-17T18:03:11.406317Z","shell.execute_reply":"2024-06-17T18:03:25.185851Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  18.7M      0  0:00:04  0:00:04 --:--:-- 19.3M\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 16\nimdb_train = keras.utils.text_dataset_from_directory(\n    \"aclImdb/train\",\n    batch_size=BATCH_SIZE,\n)\nimdb_test = keras.utils.text_dataset_from_directory(\n    \"aclImdb/test\",\n    batch_size=BATCH_SIZE,\n)\n\n# Inspect first review\n# Format is (review text tensor, label tensor)\nprint(imdb_train.unbatch().take(1).get_single_element())","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:03:25.189930Z","iopub.execute_input":"2024-06-17T18:03:25.190268Z","iopub.status.idle":"2024-06-17T18:03:31.268031Z","shell.execute_reply.started":"2024-06-17T18:03:25.190239Z","shell.execute_reply":"2024-06-17T18:03:31.266964Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 25000 files belonging to 2 classes.\nFound 25000 files belonging to 2 classes.\n(<tf.Tensor: shape=(), dtype=string, numpy=b'I have just watched the whole 6 episodes on DVD. The acting throughout is excellent - no question. There was not quite enough action for me I must say. No real suspense as such, just plenty of first class character development. Nothing like Tinker Tailor in terms of \"whodunnit\". If you like a good story slowly and carefully told then this is for you. Peter Egan as the lead Magnus Pym is excellent.<br /><br />The film portrayed the life of a traitor. A man who should have been a loyal member of the British Intelligence Service but who was so damaged psychologically by his unhappy childhood that deception became his way of life in all things. As a child he adored his father but his father was exposed time & time again as a crook and a con man. Pym betrayed not for ideology or money but because he needed to deceive those closest to him (wife, son, mentor). Pym is fatally damaged by his father\\'s influence - it has eaten his moral fibre away. He has no real love or loyalty in him.<br /><br />Heavy psychological stuff and not many light moments in the 6 hour series. Very well done though.'>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n","output_type":"stream"}]},{"cell_type":"code","source":"classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n# Note: batched inputs expected so must wrap string in iterable\nclassifier.predict([\"I love modular workflows in keras-nlp!\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:03:31.269401Z","iopub.execute_input":"2024-06-17T18:03:31.269722Z","iopub.status.idle":"2024-06-17T18:03:50.274311Z","shell.execute_reply.started":"2024-06-17T18:03:31.269688Z","shell.execute_reply":"2024-06-17T18:03:50.273327Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 84 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 0 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\nAttaching 'model.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996ms/step\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([[-1.289,  1.395]], dtype=float16)"},"metadata":{}}]},{"cell_type":"code","source":"classifier.evaluate(imdb_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:03:50.275456Z","iopub.execute_input":"2024-06-17T18:03:50.275772Z","iopub.status.idle":"2024-06-17T18:04:02.692580Z","shell.execute_reply.started":"2024-06-17T18:03:50.275745Z","shell.execute_reply":"2024-06-17T18:04:02.691671Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.4540 - sparse_categorical_accuracy: 0.7888\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[0.45500457286834717, 0.7902799844741821]"},"metadata":{}}]},{"cell_type":"code","source":"classifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_tiny_en_uncased\",\n    num_classes=2,\n)\nclassifier.fit(\n    imdb_train,\n    validation_data=imdb_test,\n    epochs=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:04:02.693670Z","iopub.execute_input":"2024-06-17T18:04:02.693966Z","iopub.status.idle":"2024-06-17T18:05:02.432601Z","shell.execute_reply.started":"2024-06-17T18:04:02.693941Z","shell.execute_reply":"2024-06-17T18:05:02.431580Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 25ms/step - loss: 0.5276 - sparse_categorical_accuracy: 0.7251 - val_loss: 0.3103 - val_sparse_categorical_accuracy: 0.8677\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7ab3ac6a7760>"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\n\npreprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n    \"bert_tiny_en_uncased\",\n    sequence_length=512,\n)\n\n# Apply the preprocessor to every sample of train and test data using `map()`.\n# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n# https://www.tensorflow.org/guide/data_performance for details.\n\n# Note: only call `cache()` if you training data fits in CPU memory!\nimdb_train_cached = (\n    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\nimdb_test_cached = (\n    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\n\nclassifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_tiny_en_uncased\", preprocessor=None, num_classes=2\n)\nclassifier.fit(\n    imdb_train_cached,\n    validation_data=imdb_test_cached,\n    epochs=3,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:05:02.433951Z","iopub.execute_input":"2024-06-17T18:05:02.434265Z","iopub.status.idle":"2024-06-17T18:06:57.717391Z","shell.execute_reply.started":"2024-06-17T18:05:02.434237Z","shell.execute_reply":"2024-06-17T18:06:57.716533Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - loss: 0.5210 - sparse_categorical_accuracy: 0.7229 - val_loss: 0.3198 - val_sparse_categorical_accuracy: 0.8625\nEpoch 2/3\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 18ms/step - loss: 0.2914 - sparse_categorical_accuracy: 0.8785 - val_loss: 0.2841 - val_sparse_categorical_accuracy: 0.8800\nEpoch 3/3\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - loss: 0.2143 - sparse_categorical_accuracy: 0.9198 - val_loss: 0.2935 - val_sparse_categorical_accuracy: 0.8808\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7ab3ac4ed6f0>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_tiny_en_uncased\")\ntokenizer([\"I love modular workflows!\", \"Libraries over frameworks!\"])\n\n# Write your own packer or use one of our `Layers`\npacker = keras_nlp.layers.MultiSegmentPacker(\n    start_value=tokenizer.cls_token_id,\n    end_value=tokenizer.sep_token_id,\n    # Note: This cannot be longer than the preset's `sequence_length`, and there\n    # is no check for a custom preprocessor!\n    sequence_length=64,\n)\n\n\n# This function that takes a text sample `x` and its\n# corresponding label `y` as input and converts the\n# text into a format suitable for input into a BERT model.\ndef preprocessor(x, y):\n    token_ids, segment_ids = packer(tokenizer(x))\n    x = {\n        \"token_ids\": token_ids,\n        \"segment_ids\": segment_ids,\n        \"padding_mask\": token_ids != 0,\n    }\n    return x, y\n\n\nimdb_train_preprocessed = imdb_train.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n    tf.data.AUTOTUNE\n)\nimdb_test_preprocessed = imdb_test.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n    tf.data.AUTOTUNE\n)\n\n# Preprocessed example\nprint(imdb_train_preprocessed.unbatch().take(1).get_single_element())","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:06:57.718535Z","iopub.execute_input":"2024-06-17T18:06:57.718849Z","iopub.status.idle":"2024-06-17T18:07:03.768490Z","shell.execute_reply.started":"2024-06-17T18:06:57.718817Z","shell.execute_reply":"2024-06-17T18:07:03.767564Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"({'token_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\narray([  101,  2023,  2003,  2763,  1996,  2087, 11771,  1010,  4788,\n        1998, 11809,  2143,  1045,  2031,  2464,  2197,  2095,  1012,\n        1996,  5436,  2008,  2001,  3214,  2000,  2031,  2070,  9569,\n        5919,  6003,  2000,  2033,  2004,  1037,  2200,  2919,  8892,\n        6100,  1997,  1996,  8185,  1010,  2007,  7564,  1997,   100,\n        1024,  1996, 10459,  4702,  8872,  1010,  2204,  2559,  1010,\n        8317,  2135, 12491,  1010,  5777,  2007,  2010,  3282,  1012,\n         102], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n      dtype=int32)>, 'padding_mask': <tf.Tensor: shape=(64,), dtype=bool, numpy=\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True])>}, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\nbackbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")\n\nimdb_train_preprocessed = (\n    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\nimdb_test_preprocessed = (\n    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\n\nbackbone.trainable = False\ninputs = backbone.input\nsequence = backbone(inputs)[\"sequence_output\"]\nfor _ in range(2):\n    sequence = keras_nlp.layers.TransformerEncoder(\n        num_heads=2,\n        intermediate_dim=512,\n        dropout=0.1,\n    )(sequence)\n# Use [CLS] token output to classify\noutputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])\n\nmodel = keras.Model(inputs, outputs)\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.AdamW(5e-5),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    jit_compile=True,\n)\nmodel.summary()\nmodel.fit(\n    imdb_train_preprocessed,\n    validation_data=imdb_test_preprocessed,\n    epochs=3,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:07:03.771398Z","iopub.execute_input":"2024-06-17T18:07:03.771673Z","iopub.status.idle":"2024-06-17T18:09:18.126530Z","shell.execute_reply.started":"2024-06-17T18:07:03.771649Z","shell.execute_reply":"2024-06-17T18:09:18.125554Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ segment_ids         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bert_backbone       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │  \u001b[38;5;34m4,385,920\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mBertBackbone\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │            │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m198,272\u001b[0m │ bert_backbone[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m198,272\u001b[0m │ transformer_enco… │\n│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item_4          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ transformer_enco… │\n│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m258\u001b[0m │ get_item_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ segment_ids         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bert_backbone       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ bert_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ transformer_enco… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item_4          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_enco… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ get_item_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,782,722\u001b[0m (18.24 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,782,722</span> (18.24 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m396,802\u001b[0m (1.51 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">396,802</span> (1.51 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,385,920\u001b[0m (16.73 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> (16.73 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/3\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 29ms/step - loss: 0.6341 - sparse_categorical_accuracy: 0.6522 - val_loss: 0.5307 - val_sparse_categorical_accuracy: 0.7345\nEpoch 2/3\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 22ms/step - loss: 0.5465 - sparse_categorical_accuracy: 0.7228 - val_loss: 0.5190 - val_sparse_categorical_accuracy: 0.7424\nEpoch 3/3\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 22ms/step - loss: 0.5152 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.4448 - val_sparse_categorical_accuracy: 0.7925\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7ab39c5e6230>"},"metadata":{}}]},{"cell_type":"code","source":"# All BERT `en` models have the same vocabulary, so reuse preprocessor from\n# \"bert_tiny_en_uncased\"\npreprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n    \"bert_tiny_en_uncased\",\n    sequence_length=256,\n)\npacker = preprocessor.packer\ntokenizer = preprocessor.tokenizer\n\n# keras.Layer to replace some input tokens with the \"[MASK]\" token\nmasker = keras_nlp.layers.MaskedLMMaskGenerator(\n    vocabulary_size=tokenizer.vocabulary_size(),\n    mask_selection_rate=0.25,\n    mask_selection_length=64,\n    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n    unselectable_token_ids=[\n        tokenizer.token_to_id(x) for x in [\"[CLS]\", \"[PAD]\", \"[SEP]\"]\n    ],\n)\n\n\ndef preprocess(inputs, label):\n    inputs = preprocessor(inputs)\n    masked_inputs = masker(inputs[\"token_ids\"])\n    # Split the masking layer outputs into a (features, labels, and weights)\n    # tuple that we can use with keras.Model.fit().\n    features = {\n        \"token_ids\": masked_inputs[\"token_ids\"],\n        \"segment_ids\": inputs[\"segment_ids\"],\n        \"padding_mask\": inputs[\"padding_mask\"],\n        \"mask_positions\": masked_inputs[\"mask_positions\"],\n    }\n    labels = masked_inputs[\"mask_ids\"]\n    weights = masked_inputs[\"mask_weights\"]\n    return features, labels, weights\n\n\npretrain_ds = imdb_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n    tf.data.AUTOTUNE\n)\npretrain_val_ds = imdb_test.map(\n    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n).prefetch(tf.data.AUTOTUNE)\n\n# Tokens with ID 103 are \"masked\"\nprint(pretrain_ds.unbatch().take(1).get_single_element())","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:09:18.127891Z","iopub.execute_input":"2024-06-17T18:09:18.128258Z","iopub.status.idle":"2024-06-17T18:09:26.811811Z","shell.execute_reply.started":"2024-06-17T18:09:18.128223Z","shell.execute_reply":"2024-06-17T18:09:26.810807Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"({'token_ids': <tf.Tensor: shape=(256,), dtype=int32, numpy=\narray([  101,  2045,  1005,  1055,   103,   103,   103,  2843,   103,\n        2360,   103,   103,  7136,  3203,  1012,  2009,  1005,  1055,\n       19741,  2438,  1010,  2021,  2009,   103,  2210,  2062,  2084,\n         103,   103,  2002,  2923,  2143,  2013,  1996, 25984,  1012,\n         103,  3185,   103,   103,   103, 12266, 24846,  2004,  7916,\n        2004,  2009,   103,  2015,  2000,  2022,  1012,  1996,  5436,\n        2003,   103,  2128,  7913,  4215,  1997, 10521,  5458,   103,\n        9270, 13742,  9994,  2008,   103,   103,   103,   103,  2000,\n        2331,  1012,  3272,  1999,  1996,  2553,  1997,  5869,  7136,\n        3203,  1010,  1045,  2228,  1996, 13742,  5436,   103,  2881,\n        2011,  1037,  1017,  2095,  1011,   103,   103,  1996,  2933,\n        7336,  2093,  2308,  2028,  2000,   103,   103,  9623, 22740,\n         103,  1998,  1999,   103,  3193, 15213,  1996,  2648,  1997,\n        1996,  9661, 25022,  7874,  2311,  1010,  2028,  2000, 15305,\n         103, 24693,  6775,  2069,  2000,  6271,  2014,   103,   103,\n        1996,  2034,  4495,  1010,   103,  2028,  2000,   103,  2105,\n       14944,  2014, 28691,  1012,  2008,  1005,  1055,  3492,   103,\n        2009,   103, 17796,  1010,  9616,  1029,   103,  2084,   103,\n        8799,  1998,  2014, 17289, 12682,   103,  1996,  2060,   103,\n        2920,  1999,   103,  5436,  4995,  1005,  1056,  3391, 13432,\n         103,  5869,  7136,  3203,  2522,  1011,  3340,  6990, 21311,\n        1012,  2043,  2025, 22195,   103, 21722,  1010,  2010,  6624,\n        1999,   103,  3185,  2003, 11548,  2011,  2028,  1997,  1996,\n        2087, 10041,   103,  3282,  9590,  2412,  2404,   103,  2143,\n        1012,   103,  7987,  1013,  1028,   103,   103,  1013,  1028,\n        1045,  2428,  2359,  2000,  2066,  2023,  3185,  1012,  2009,\n        2515,  2031,   103,   103, 14198,   103,  2009,  2008,  1045,\n         103,  5959,  1998,  2070,  3835,  7171,  1997,  5869,  7136,\n       12800,  3339,  1012,   102], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(256,), dtype=int32, numpy=\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>, 'padding_mask': <tf.Tensor: shape=(256,), dtype=bool, numpy=\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True])>, 'mask_positions': <tf.Tensor: shape=(64,), dtype=int64, numpy=\narray([  4,   5,   6,   8,  10,  11,  23,  27,  28,  34,  36,  37,  38,\n        39,  40,  42,  47,  55,  60,  62,  67,  68,  69,  70,  88,  95,\n        96, 104, 105, 108, 111, 113, 118, 125, 126, 127, 128, 133, 134,\n       139, 142, 152, 154, 159, 161, 167, 170, 173, 180, 188, 193, 194,\n       199, 209, 214, 217, 221, 222, 227, 236, 237, 238, 239, 243])>}, <tf.Tensor: shape=(64,), dtype=int32, numpy=\narray([ 2428,  2025,  1037,  2000,  2055,  5869,  2003,  1037, 10634,\n       17549,  1996,  3185,  2003,  4445,  2004,  4496, 29453,  1037,\n        1996,  2214,  1005,  1055,  2042,  2589,  2001,  2214,  1012,\n        4895,  2638,  2135,  5810,  4094,  9661, 13382,  2004,  1037,\n       13877,  3104,  2012,  1998,  3233,  2172,  1012,  2060, 11894,\n        1010,  2308,  1996,  1012, 21311,  2075,  8799,  1996,  2594,\n        2006,  1026,  1026,  7987,  2359,  2008, 17549,  2514,  2000,\n        2467], dtype=int32)>, <tf.Tensor: shape=(64,), dtype=float16, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float16)>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# BERT backbone\nbackbone = keras_nlp.models.BertBackbone(\n    vocabulary_size=tokenizer.vocabulary_size(),\n    num_layers=2,\n    num_heads=2,\n    hidden_dim=128,\n    intermediate_dim=512,\n)\n\n# Language modeling head\nmlm_head = keras_nlp.layers.MaskedLMHead(\n    token_embedding=backbone.token_embedding,\n)\n\ninputs = {\n    \"token_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"token_ids\"),\n    \"segment_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"segment_ids\"),\n    \"padding_mask\": keras.Input(shape=(None,), dtype=tf.int32, name=\"padding_mask\"),\n    \"mask_positions\": keras.Input(shape=(None,), dtype=tf.int32, name=\"mask_positions\"),\n}\n\n# Encoded token sequence\nsequence = backbone(inputs)[\"sequence_output\"]\n\n# Predict an output word for each masked input token.\n# We use the input token embedding to project from our encoded vectors to\n# vocabulary logits, which has been shown to improve training efficiency.\noutputs = mlm_head(sequence, mask_positions=inputs[\"mask_positions\"])\n\n# Define and compile our pretraining model.\npretraining_model = keras.Model(inputs, outputs)\npretraining_model.summary()\npretraining_model.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.AdamW(learning_rate=5e-4),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    jit_compile=True,\n)\n\n# Pretrain on IMDB dataset\npretraining_model.fit(\n    pretrain_ds,\n    validation_data=pretrain_val_ds,\n    epochs=3,  # Increase to 6 for higher accuracy\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:09:26.813304Z","iopub.execute_input":"2024-06-17T18:09:26.814018Z","iopub.status.idle":"2024-06-17T18:11:50.771303Z","shell.execute_reply.started":"2024-06-17T18:09:26.813976Z","shell.execute_reply":"2024-06-17T18:11:50.770394Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ mask_positions      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ padding_mask        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ segment_ids         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bert_backbone       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │  \u001b[38;5;34m4,385,920\u001b[0m │ mask_positions[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mBertBackbone\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│                     │                   │            │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ masked_lm_head      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m3,954,106\u001b[0m │ bert_backbone[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mMaskedLMHead\u001b[0m)      │ \u001b[38;5;34m30522\u001b[0m)            │            │ mask_positions[\u001b[38;5;34m0\u001b[0m… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ mask_positions      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ padding_mask        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ segment_ids         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bert_backbone       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> │ mask_positions[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │                   │            │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ masked_lm_head      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,954,106</span> │ bert_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedLMHead</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">30522</span>)            │            │ mask_positions[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,433,210\u001b[0m (16.91 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,433,210</span> (16.91 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,433,210\u001b[0m (16.91 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,433,210</span> (16.91 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/3\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 34ms/step - loss: 5.6174 - sparse_categorical_accuracy: 0.0628 - val_loss: 4.9786 - val_sparse_categorical_accuracy: 0.1178\nEpoch 2/3\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 25ms/step - loss: 4.9975 - sparse_categorical_accuracy: 0.1200 - val_loss: 4.8815 - val_sparse_categorical_accuracy: 0.1301\nEpoch 3/3\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 25ms/step - loss: 4.8775 - sparse_categorical_accuracy: 0.1377 - val_loss: 4.5271 - val_sparse_categorical_accuracy: 0.1987\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7ab35483c370>"},"metadata":{}}]},{"cell_type":"code","source":"vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n    imdb_train.map(lambda x, y: x),\n    vocabulary_size=20_000,\n    lowercase=True,\n    strip_accents=True,\n    reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[MASK]\", \"[UNK]\"],\n)\ntokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n    vocabulary=vocab,\n    lowercase=True,\n    strip_accents=True,\n    oov_token=\"[UNK]\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:11:50.772462Z","iopub.execute_input":"2024-06-17T18:11:50.772758Z","iopub.status.idle":"2024-06-17T18:16:28.437330Z","shell.execute_reply.started":"2024-06-17T18:11:50.772732Z","shell.execute_reply":"2024-06-17T18:16:28.435973Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"packer = keras_nlp.layers.StartEndPacker(\n    start_value=tokenizer.token_to_id(\"[START]\"),\n    end_value=tokenizer.token_to_id(\"[END]\"),\n    pad_value=tokenizer.token_to_id(\"[PAD]\"),\n    sequence_length=512,\n)\n\n\ndef preprocess(x, y):\n    token_ids = packer(tokenizer(x))\n    return token_ids, y\n\n\nimdb_preproc_train_ds = imdb_train.map(\n    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n).prefetch(tf.data.AUTOTUNE)\nimdb_preproc_val_ds = imdb_test.map(\n    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n).prefetch(tf.data.AUTOTUNE)\n\nprint(imdb_preproc_train_ds.unbatch().take(1).get_single_element())","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:16:28.438762Z","iopub.execute_input":"2024-06-17T18:16:28.439103Z","iopub.status.idle":"2024-06-17T18:16:29.684909Z","shell.execute_reply.started":"2024-06-17T18:16:28.439074Z","shell.execute_reply":"2024-06-17T18:16:29.683734Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(<tf.Tensor: shape=(512,), dtype=int32, numpy=\narray([    1,    52,   122,   172,   363,   111,   107,   115,   111,\n        2244,   181,   105,   103,   277,  1390,   102,   265,   110,\n         105,   109,   124,   101,    99,   470, 18850,    19,    52,\n         519,   354,   105,   117,   377,   126,    99,   225,   101,\n         329,   375,   105,   103,    44,   150,   461,   449,   113,\n          99,   158,   103,   365,   284,  5788,  2811, 13537,   108,\n         120,   677,   106,  1180,   101,    99,  1854,    19,  3112,\n        5432,   103,    44,  2309,   130,   103,  3786,   128,    44,\n        1450,   101,  4280,   231,   148,   103,   876,   106,  1328,\n         942,    19,   127,   289,   135,   178,    99,  2811,  3112,\n         103, 11177,   128,   144,   741,  2329,   130,  2165,   411,\n           7,    99,  1854,     7,   148,   142,   102,   233,   245,\n         125,  4310,   280,   465,   127,   120,   740,    19, 10261,\n         139,   120, 12889,   151,    19,    99,  1854,  6083,  2038,\n        2263,   106,    44,   624,  1491,    19,   898,  1321,   784,\n          17,   103,    99,  1854,  2324,    17,   130,   366,   178,\n          99,  2811,   102,   450,   100,  2395,   135,    17,     2,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0],\n      dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"token_id_input = keras.Input(\n    shape=(None,),\n    dtype=\"int32\",\n    name=\"token_ids\",\n)\noutputs = keras_nlp.layers.TokenAndPositionEmbedding(\n    vocabulary_size=len(vocab),\n    sequence_length=packer.sequence_length,\n    embedding_dim=64,\n)(token_id_input)\noutputs = keras_nlp.layers.TransformerEncoder(\n    num_heads=2,\n    intermediate_dim=128,\n    dropout=0.1,\n)(outputs)\n# Use \"[START]\" token to classify\noutputs = keras.layers.Dense(2)(outputs[:, 0, :])\nmodel = keras.Model(\n    inputs=token_id_input,\n    outputs=outputs,\n)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:16:29.686138Z","iopub.execute_input":"2024-06-17T18:16:29.686449Z","iopub.status.idle":"2024-06-17T18:16:30.351489Z","shell.execute_reply.started":"2024-06-17T18:16:29.686420Z","shell.execute_reply":"2024-06-17T18:16:30.350570Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │     \u001b[38;5;34m1,260,544\u001b[0m │\n│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_encoder_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m33,472\u001b[0m │\n│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ get_item_6 (\u001b[38;5;33mGetItem\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,260,544</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_encoder_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,472</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ get_item_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,294,146\u001b[0m (4.94 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,294,146</span> (4.94 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,294,146\u001b[0m (4.94 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,294,146</span> (4.94 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"model.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.AdamW(5e-5),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    jit_compile=True,\n)\nmodel.fit(\n    imdb_preproc_train_ds,\n    validation_data=imdb_preproc_val_ds,\n    epochs=3,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T18:16:30.352820Z","iopub.execute_input":"2024-06-17T18:16:30.353195Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 14ms/step - loss: 0.7406 - sparse_categorical_accuracy: 0.5258 - val_loss: 0.5459 - val_sparse_categorical_accuracy: 0.6913\nEpoch 2/3\n","output_type":"stream"}]}]}