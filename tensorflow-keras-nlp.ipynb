{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"modelInstanceVersion","sourceId":40168,"databundleVersionId":8367325,"modelInstanceId":4723}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://keras.io/guides/keras_nlp/getting_started/","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade keras-nlp\n!pip install -q --upgrade keras  # Upgrade to Keras 3.","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:55:33.183916Z","iopub.execute_input":"2024-06-16T15:55:33.184343Z","iopub.status.idle":"2024-06-16T15:56:00.792558Z","shell.execute_reply.started":"2024-06-16T15:55:33.184314Z","shell.execute_reply":"2024-06-16T15:56:00.791337Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n\nimport keras_nlp\nimport keras\n\n# Use mixed precision to speed up all training in this guide.\nkeras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:56:13.028440Z","iopub.execute_input":"2024-06-16T15:56:13.029161Z","iopub.status.idle":"2024-06-16T15:56:13.034929Z","shell.execute_reply.started":"2024-06-16T15:56:13.029126Z","shell.execute_reply":"2024-06-16T15:56:13.033688Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz\n!# Remove unsupervised examples\n!rm -r aclImdb/train/unsup","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:56:15.754689Z","iopub.execute_input":"2024-06-16T15:56:15.755107Z","iopub.status.idle":"2024-06-16T15:56:44.292204Z","shell.execute_reply.started":"2024-06-16T15:56:15.755076Z","shell.execute_reply":"2024-06-16T15:56:44.290689Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  4830k      0  0:00:17  0:00:17 --:--:-- 10.8M\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 16\nimdb_train = keras.utils.text_dataset_from_directory(\n    \"aclImdb/train\",\n    batch_size=BATCH_SIZE,\n)\nimdb_test = keras.utils.text_dataset_from_directory(\n    \"aclImdb/test\",\n    batch_size=BATCH_SIZE,\n)\n\n# Inspect first review\n# Format is (review text tensor, label tensor)\nprint(imdb_train.unbatch().take(1).get_single_element())","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:56:44.294634Z","iopub.execute_input":"2024-06-16T15:56:44.295019Z","iopub.status.idle":"2024-06-16T15:56:47.631090Z","shell.execute_reply.started":"2024-06-16T15:56:44.294983Z","shell.execute_reply":"2024-06-16T15:56:47.629613Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Found 25000 files belonging to 2 classes.\nFound 25000 files belonging to 2 classes.\n(<tf.Tensor: shape=(), dtype=string, numpy=b\"After reading several good reviews as well as hearing nice things about it by word of mouth I decided to rent Come Undone. I must say I was rather disappointed. The story was hard to follow because the film is set as a series of flashbacks between the present and recent past that are very poorly executed. The characters, despite the actors best efforts are flat and uninteresting. The sex is and nudity are more explicit than they need to be. I've never seen a film where they seemed so unnecessary to the plot. The ending is very anti-climatic and leaves many unanswered questions to a story line that wasn't explained well to begin with. In my opinion, a waste of time.\">, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n# Note: batched inputs expected so must wrap string in iterable\nclassifier.predict([\"I love modular workflows in keras-nlp!\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:57:09.387359Z","iopub.execute_input":"2024-06-16T15:57:09.387809Z","iopub.status.idle":"2024-06-16T15:57:28.429022Z","shell.execute_reply.started":"2024-06-16T15:57:09.387777Z","shell.execute_reply":"2024-06-16T15:57:28.427871Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'task.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 84 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 0 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\nAttaching 'model.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased_sst2/4' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([[-1.287,  1.393]], dtype=float16)"},"metadata":{}}]},{"cell_type":"code","source":"classifier.evaluate(imdb_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T15:57:46.126081Z","iopub.execute_input":"2024-06-16T15:57:46.126475Z","iopub.status.idle":"2024-06-16T16:09:27.945625Z","shell.execute_reply.started":"2024-06-16T15:57:46.126443Z","shell.execute_reply":"2024-06-16T16:09:27.944436Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 447ms/step - loss: 0.4592 - sparse_categorical_accuracy: 0.7888\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[0.4550009071826935, 0.7903599739074707]"},"metadata":{}}]},{"cell_type":"code","source":"classifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_tiny_en_uncased\",\n    num_classes=2,\n)\nclassifier.fit(\n    imdb_train,\n    validation_data=imdb_test,\n    epochs=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:26:33.808276Z","iopub.execute_input":"2024-06-16T16:26:33.808752Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/bert/keras/bert_tiny_en_uncased/2' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1328/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m5:33\u001b[0m 1s/step - loss: 0.5302 - sparse_categorical_accuracy: 0.7189","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\n\npreprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n    \"bert_tiny_en_uncased\",\n    sequence_length=512,\n)\n\n# Apply the preprocessor to every sample of train and test data using `map()`.\n# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n# https://www.tensorflow.org/guide/data_performance for details.\n\n# Note: only call `cache()` if you training data fits in CPU memory!\nimdb_train_cached = (\n    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\nimdb_test_cached = (\n    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\n\nclassifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_tiny_en_uncased\", preprocessor=None, num_classes=2\n)\nclassifier.fit(\n    imdb_train_cached,\n    validation_data=imdb_test_cached,\n    epochs=3,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_tiny_en_uncased\")\ntokenizer([\"I love modular workflows!\", \"Libraries over frameworks!\"])\n\n# Write your own packer or use one of our `Layers`\npacker = keras_nlp.layers.MultiSegmentPacker(\n    start_value=tokenizer.cls_token_id,\n    end_value=tokenizer.sep_token_id,\n    # Note: This cannot be longer than the preset's `sequence_length`, and there\n    # is no check for a custom preprocessor!\n    sequence_length=64,\n)\n\n\n# This function that takes a text sample `x` and its\n# corresponding label `y` as input and converts the\n# text into a format suitable for input into a BERT model.\ndef preprocessor(x, y):\n    token_ids, segment_ids = packer(tokenizer(x))\n    x = {\n        \"token_ids\": token_ids,\n        \"segment_ids\": segment_ids,\n        \"padding_mask\": token_ids != 0,\n    }\n    return x, y\n\n\nimdb_train_preprocessed = imdb_train.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n    tf.data.AUTOTUNE\n)\nimdb_test_preprocessed = imdb_test.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n    tf.data.AUTOTUNE\n)\n\n# Preprocessed example\nprint(imdb_train_preprocessed.unbatch().take(1).get_single_element())","metadata":{},"execution_count":null,"outputs":[]}]}