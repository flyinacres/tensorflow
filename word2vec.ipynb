{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.tensorflow.org/text/tutorials/word2vec","metadata":{}},{"cell_type":"code","source":"import io\nimport re\nimport string\nimport tqdm\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:44.003088Z","iopub.execute_input":"2024-07-21T05:02:44.003473Z","iopub.status.idle":"2024-07-21T05:02:56.629392Z","shell.execute_reply.started":"2024-07-21T05:02:44.003442Z","shell.execute_reply":"2024-07-21T05:02:56.628443Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-21 05:02:45.927692: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-21 05:02:45.927840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-21 05:02:46.069920: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the TensorBoard notebook extension\n%load_ext tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.630974Z","iopub.execute_input":"2024-07-21T05:02:56.631564Z","iopub.status.idle":"2024-07-21T05:02:56.654842Z","shell.execute_reply.started":"2024-07-21T05:02:56.631534Z","shell.execute_reply":"2024-07-21T05:02:56.653814Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.656311Z","iopub.execute_input":"2024-07-21T05:02:56.656619Z","iopub.status.idle":"2024-07-21T05:02:56.667581Z","shell.execute_reply.started":"2024-07-21T05:02:56.656592Z","shell.execute_reply":"2024-07-21T05:02:56.666620Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sentence = \"The wide road shimmered in the hot sun\"\ntokens = list(sentence.lower().split())\nprint(len(tokens))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.669805Z","iopub.execute_input":"2024-07-21T05:02:56.670134Z","iopub.status.idle":"2024-07-21T05:02:56.677848Z","shell.execute_reply.started":"2024-07-21T05:02:56.670101Z","shell.execute_reply":"2024-07-21T05:02:56.676824Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"8\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab, index = {}, 1  # start indexing from 1\nvocab['<pad>'] = 0  # add a padding token\nfor token in tokens:\n  if token not in vocab:\n    vocab[token] = index\n    index += 1\nvocab_size = len(vocab)\nprint(vocab)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.678941Z","iopub.execute_input":"2024-07-21T05:02:56.679279Z","iopub.status.idle":"2024-07-21T05:02:56.687408Z","shell.execute_reply.started":"2024-07-21T05:02:56.679254Z","shell.execute_reply":"2024-07-21T05:02:56.686384Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n","output_type":"stream"}]},{"cell_type":"code","source":"inverse_vocab = {index: token for token, index in vocab.items()}\nprint(inverse_vocab)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.688604Z","iopub.execute_input":"2024-07-21T05:02:56.688898Z","iopub.status.idle":"2024-07-21T05:02:56.697166Z","shell.execute_reply.started":"2024-07-21T05:02:56.688874Z","shell.execute_reply":"2024-07-21T05:02:56.696166Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n","output_type":"stream"}]},{"cell_type":"code","source":"example_sequence = [vocab[word] for word in tokens]\nprint(example_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.698311Z","iopub.execute_input":"2024-07-21T05:02:56.698666Z","iopub.status.idle":"2024-07-21T05:02:56.706079Z","shell.execute_reply.started":"2024-07-21T05:02:56.698634Z","shell.execute_reply":"2024-07-21T05:02:56.705091Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[1, 2, 3, 4, 5, 1, 6, 7]\n","output_type":"stream"}]},{"cell_type":"code","source":"window_size = 2\npositive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n      example_sequence,\n      vocabulary_size=vocab_size,\n      window_size=window_size,\n      negative_samples=0)\nprint(len(positive_skip_grams))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.707963Z","iopub.execute_input":"2024-07-21T05:02:56.708734Z","iopub.status.idle":"2024-07-21T05:02:56.714989Z","shell.execute_reply.started":"2024-07-21T05:02:56.708707Z","shell.execute_reply":"2024-07-21T05:02:56.714055Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"26\n","output_type":"stream"}]},{"cell_type":"code","source":"for target, context in positive_skip_grams[:5]:\n  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.716176Z","iopub.execute_input":"2024-07-21T05:02:56.716470Z","iopub.status.idle":"2024-07-21T05:02:56.724064Z","shell.execute_reply.started":"2024-07-21T05:02:56.716446Z","shell.execute_reply":"2024-07-21T05:02:56.723121Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(4, 1): (shimmered, the)\n(3, 1): (road, the)\n(1, 7): (the, sun)\n(7, 1): (sun, the)\n(5, 6): (in, hot)\n","output_type":"stream"}]},{"cell_type":"code","source":"target_word, context_word = positive_skip_grams[0]\n\n# Set the number of negative samples per positive context.\nnum_ns = 4\n\ncontext_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\nnegative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n    true_classes=context_class,  # class that should be sampled as 'positive'\n    num_true=1,  # each positive skip-gram has 1 positive context class\n    num_sampled=num_ns,  # number of negative context words to sample\n    unique=True,  # all the negative samples should be unique\n    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n    seed=SEED,  # seed for reproducibility\n    name=\"negative_sampling\"  # name of this operation\n)\nprint(negative_sampling_candidates)\nprint([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.728487Z","iopub.execute_input":"2024-07-21T05:02:56.728784Z","iopub.status.idle":"2024-07-21T05:02:56.769741Z","shell.execute_reply.started":"2024-07-21T05:02:56.728760Z","shell.execute_reply":"2024-07-21T05:02:56.768802Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n['wide', 'the', 'shimmered', 'road']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reduce a dimension so you can use concatenation (in the next step).\nsqueezed_context_class = tf.squeeze(context_class, 1)\n\n# Concatenate a positive context word with negative sampled words.\ncontext = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n\n# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\nlabel = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\ntarget = target_word","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.770698Z","iopub.execute_input":"2024-07-21T05:02:56.770970Z","iopub.status.idle":"2024-07-21T05:02:56.777829Z","shell.execute_reply.started":"2024-07-21T05:02:56.770947Z","shell.execute_reply":"2024-07-21T05:02:56.776900Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(f\"target_index    : {target}\")\nprint(f\"target_word     : {inverse_vocab[target_word]}\")\nprint(f\"context_indices : {context}\")\nprint(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\nprint(f\"label           : {label}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.779084Z","iopub.execute_input":"2024-07-21T05:02:56.779413Z","iopub.status.idle":"2024-07-21T05:02:56.791040Z","shell.execute_reply.started":"2024-07-21T05:02:56.779385Z","shell.execute_reply":"2024-07-21T05:02:56.789973Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"target_index    : 4\ntarget_word     : shimmered\ncontext_indices : [1 2 1 4 3]\ncontext_words   : ['the', 'wide', 'the', 'shimmered', 'road']\nlabel           : [1 0 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"target  :\", target)\nprint(\"context :\", context)\nprint(\"label   :\", label)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.792212Z","iopub.execute_input":"2024-07-21T05:02:56.792964Z","iopub.status.idle":"2024-07-21T05:02:56.798472Z","shell.execute_reply.started":"2024-07-21T05:02:56.792934Z","shell.execute_reply":"2024-07-21T05:02:56.797377Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"target  : 4\ncontext : tf.Tensor([1 2 1 4 3], shape=(5,), dtype=int64)\nlabel   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\nprint(sampling_table)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.799746Z","iopub.execute_input":"2024-07-21T05:02:56.800122Z","iopub.status.idle":"2024-07-21T05:02:56.807782Z","shell.execute_reply.started":"2024-07-21T05:02:56.800081Z","shell.execute_reply":"2024-07-21T05:02:56.806823Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n 0.01212381 0.01347162 0.01474487 0.0159558 ]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generates skip-gram pairs with negative sampling for a list of sequences\n# (int-encoded sentences) based on window size, number of negative samples\n# and vocabulary size.\ndef generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n  # Elements of each training example are appended to these lists.\n  targets, contexts, labels = [], [], []\n\n  # Build the sampling table for `vocab_size` tokens.\n  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n\n  # Iterate over all sequences (sentences) in the dataset.\n  for sequence in tqdm.tqdm(sequences):\n\n    # Generate positive skip-gram pairs for a sequence (sentence).\n    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n          sequence,\n          vocabulary_size=vocab_size,\n          sampling_table=sampling_table,\n          window_size=window_size,\n          negative_samples=0)\n    \n      # Iterate over each positive skip-gram pair to produce training examples\n    # with a positive context word and negative samples.\n    for target_word, context_word in positive_skip_grams:\n      context_class = tf.expand_dims(\n          tf.constant([context_word], dtype=\"int64\"), 1)\n      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n          true_classes=context_class,\n          num_true=1,\n          num_sampled=num_ns,\n          unique=True,\n          range_max=vocab_size,\n          seed=seed,\n          name=\"negative_sampling\")\n\n      # Build context and label vectors (for one target word)\n      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n\n      # Append each element from the training example to global lists.\n      targets.append(target_word)\n      contexts.append(context)\n      labels.append(label)\n\n  return targets, contexts, labels","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.808936Z","iopub.execute_input":"2024-07-21T05:02:56.809309Z","iopub.status.idle":"2024-07-21T05:02:56.818584Z","shell.execute_reply.started":"2024-07-21T05:02:56.809278Z","shell.execute_reply":"2024-07-21T05:02:56.817567Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Now to try an entire dataset (corpus)","metadata":{}},{"cell_type":"code","source":"path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:56.819760Z","iopub.execute_input":"2024-07-21T05:02:56.820070Z","iopub.status.idle":"2024-07-21T05:02:57.897244Z","shell.execute_reply.started":"2024-07-21T05:02:56.820040Z","shell.execute_reply":"2024-07-21T05:02:57.896201Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(path_to_file) as f:\n  lines = f.read().splitlines()\nfor line in lines[:20]:\n  print(line)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:57.898727Z","iopub.execute_input":"2024-07-21T05:02:57.899061Z","iopub.status.idle":"2024-07-21T05:02:57.910859Z","shell.execute_reply.started":"2024-07-21T05:02:57.899035Z","shell.execute_reply":"2024-07-21T05:02:57.909631Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\n","output_type":"stream"}]},{"cell_type":"code","source":"text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:57.912114Z","iopub.execute_input":"2024-07-21T05:02:57.912513Z","iopub.status.idle":"2024-07-21T05:02:57.969042Z","shell.execute_reply.started":"2024-07-21T05:02:57.912477Z","shell.execute_reply":"2024-07-21T05:02:57.968199Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Now, create a custom standardization function to lowercase the text and\n# remove punctuation.\ndef custom_standardization(input_data):\n  lowercase = tf.strings.lower(input_data)\n  return tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation), '')\n\n\n# Define the vocabulary size and the number of words in a sequence.\nvocab_size = 4096\nsequence_length = 10\n\n# Use the `TextVectorization` layer to normalize, split, and map strings to\n# integers. Set the `output_sequence_length` length to pad all samples to the\n# same length.\nvectorize_layer = layers.TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size,\n    output_mode='int',\n    output_sequence_length=sequence_length)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:57.970179Z","iopub.execute_input":"2024-07-21T05:02:57.970456Z","iopub.status.idle":"2024-07-21T05:02:57.984970Z","shell.execute_reply.started":"2024-07-21T05:02:57.970433Z","shell.execute_reply":"2024-07-21T05:02:57.984063Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"vectorize_layer.adapt(text_ds.batch(1024))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:57.986230Z","iopub.execute_input":"2024-07-21T05:02:57.986540Z","iopub.status.idle":"2024-07-21T05:02:59.289099Z","shell.execute_reply.started":"2024-07-21T05:02:57.986515Z","shell.execute_reply":"2024-07-21T05:02:59.288274Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Save the created vocabulary for reference.\ninverse_vocab = vectorize_layer.get_vocabulary()\nprint(inverse_vocab[:20])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:59.290240Z","iopub.execute_input":"2024-07-21T05:02:59.290547Z","iopub.status.idle":"2024-07-21T05:02:59.309278Z","shell.execute_reply.started":"2024-07-21T05:02:59.290521Z","shell.execute_reply":"2024-07-21T05:02:59.308317Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Vectorize the data in text_ds.\ntext_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:59.310334Z","iopub.execute_input":"2024-07-21T05:02:59.310606Z","iopub.status.idle":"2024-07-21T05:02:59.406340Z","shell.execute_reply.started":"2024-07-21T05:02:59.310583Z","shell.execute_reply":"2024-07-21T05:02:59.405466Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"sequences = list(text_vector_ds.as_numpy_iterator())\nprint(len(sequences))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:02:59.407480Z","iopub.execute_input":"2024-07-21T05:02:59.407780Z","iopub.status.idle":"2024-07-21T05:03:04.475139Z","shell.execute_reply.started":"2024-07-21T05:02:59.407754Z","shell.execute_reply":"2024-07-21T05:03:04.474018Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"32777\n","output_type":"stream"}]},{"cell_type":"code","source":"for seq in sequences[:5]:\n  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:03:04.476570Z","iopub.execute_input":"2024-07-21T05:03:04.476997Z","iopub.status.idle":"2024-07-21T05:03:04.483510Z","shell.execute_reply.started":"2024-07-21T05:03:04.476961Z","shell.execute_reply":"2024-07-21T05:03:04.482224Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n","output_type":"stream"}]},{"cell_type":"code","source":"targets, contexts, labels = generate_training_data(\n    sequences=sequences,\n    window_size=2,\n    num_ns=4,\n    vocab_size=vocab_size,\n    seed=SEED)\n\ntargets = np.array(targets)\ncontexts = np.array(contexts)\nlabels = np.array(labels)\n\nprint('\\n')\nprint(f\"targets.shape: {targets.shape}\")\nprint(f\"contexts.shape: {contexts.shape}\")\nprint(f\"labels.shape: {labels.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:03:04.484718Z","iopub.execute_input":"2024-07-21T05:03:04.485016Z","iopub.status.idle":"2024-07-21T05:03:25.071686Z","shell.execute_reply.started":"2024-07-21T05:03:04.484991Z","shell.execute_reply":"2024-07-21T05:03:25.070641Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"100%|██████████| 32777/32777 [00:19<00:00, 1684.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n\ntargets.shape: (65252,)\ncontexts.shape: (65252, 5)\nlabels.shape: (65252, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 1024\nBUFFER_SIZE = 10000\ndataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:03:25.072742Z","iopub.execute_input":"2024-07-21T05:03:25.073026Z","iopub.status.idle":"2024-07-21T05:03:25.088738Z","shell.execute_reply.started":"2024-07-21T05:03:25.073003Z","shell.execute_reply":"2024-07-21T05:03:25.087750Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:03:25.089868Z","iopub.execute_input":"2024-07-21T05:03:25.090204Z","iopub.status.idle":"2024-07-21T05:03:25.107759Z","shell.execute_reply.started":"2024-07-21T05:03:25.090171Z","shell.execute_reply":"2024-07-21T05:03:25.106433Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"https://www.tensorflow.org/text/tutorials/word2vec#model_and_training","metadata":{}},{"cell_type":"code","source":"class Word2Vec(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim):\n    super(Word2Vec, self).__init__()\n    self.target_embedding = layers.Embedding(vocab_size,\n                                      embedding_dim,\n                                      name=\"w2v_embedding\")\n    self.context_embedding = layers.Embedding(vocab_size,\n                                       embedding_dim)\n\n  def call(self, pair):\n    target, context = pair\n    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n    # context: (batch, context)\n    if len(target.shape) == 2:\n      target = tf.squeeze(target, axis=1)\n    # target: (batch,)\n    word_emb = self.target_embedding(target)\n    # word_emb: (batch, embed)\n    context_emb = self.context_embedding(context)\n    # context_emb: (batch, context, embed)\n    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n    # dots: (batch, context)\n    return dots","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:04:15.206821Z","iopub.execute_input":"2024-07-21T05:04:15.207268Z","iopub.status.idle":"2024-07-21T05:04:15.215032Z","shell.execute_reply.started":"2024-07-21T05:04:15.207235Z","shell.execute_reply":"2024-07-21T05:04:15.213819Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def custom_loss(x_logit, y_true):\n      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:04:28.888307Z","iopub.execute_input":"2024-07-21T05:04:28.888979Z","iopub.status.idle":"2024-07-21T05:04:28.893509Z","shell.execute_reply.started":"2024-07-21T05:04:28.888944Z","shell.execute_reply":"2024-07-21T05:04:28.892296Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 128\nword2vec = Word2Vec(vocab_size, embedding_dim)\nword2vec.compile(optimizer='adam',\n                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n                 metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:04:43.175324Z","iopub.execute_input":"2024-07-21T05:04:43.175712Z","iopub.status.idle":"2024-07-21T05:04:43.198296Z","shell.execute_reply.started":"2024-07-21T05:04:43.175681Z","shell.execute_reply":"2024-07-21T05:04:43.197287Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:04:54.194040Z","iopub.execute_input":"2024-07-21T05:04:54.194456Z","iopub.status.idle":"2024-07-21T05:04:54.199786Z","shell.execute_reply.started":"2024-07-21T05:04:54.194424Z","shell.execute_reply":"2024-07-21T05:04:54.198550Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:05:04.084269Z","iopub.execute_input":"2024-07-21T05:05:04.085033Z","iopub.status.idle":"2024-07-21T05:05:14.771960Z","shell.execute_reply.started":"2024-07-21T05:05:04.085002Z","shell.execute_reply":"2024-07-21T05:05:14.771026Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.2214 - loss: 1.6088\nEpoch 2/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5982 - loss: 1.5887\nEpoch 3/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5979 - loss: 1.5294\nEpoch 4/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5491 - loss: 1.4406\nEpoch 5/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5646 - loss: 1.3429\nEpoch 6/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6017 - loss: 1.2459\nEpoch 7/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6393 - loss: 1.1560\nEpoch 8/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6772 - loss: 1.0735\nEpoch 9/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7107 - loss: 0.9974\nEpoch 10/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7400 - loss: 0.9270\nEpoch 11/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7656 - loss: 0.8618\nEpoch 12/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7883 - loss: 0.8017\nEpoch 13/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8074 - loss: 0.7462\nEpoch 14/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8245 - loss: 0.6952\nEpoch 15/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8393 - loss: 0.6485\nEpoch 16/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8530 - loss: 0.6058\nEpoch 17/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8653 - loss: 0.5668\nEpoch 18/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8761 - loss: 0.5312\nEpoch 19/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8849 - loss: 0.4987\nEpoch 20/20\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8937 - loss: 0.4691\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7c1f6daf3970>"},"metadata":{}}]},{"cell_type":"markdown","source":"Current invocation for tensorboard does not show on Kaggle.  I had this problem before, I need to poke around on the internet to figure out how to make it work.","metadata":{}},{"cell_type":"code","source":"#docs_infra: no_execute\n%load_ext tensorboard\n%tensorboard --logdir logs","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:08:49.181112Z","iopub.execute_input":"2024-07-21T05:08:49.182102Z","iopub.status.idle":"2024-07-21T05:08:49.195914Z","shell.execute_reply.started":"2024-07-21T05:08:49.182058Z","shell.execute_reply":"2024-07-21T05:08:49.194812Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Reusing TensorBoard on port 6006 (pid 250), started 0:03:11 ago. (Use '!kill 250' to kill it.)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-e7e3ca8772aed306\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-e7e3ca8772aed306\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\nvocab = vectorize_layer.get_vocabulary()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:06:44.838955Z","iopub.execute_input":"2024-07-21T05:06:44.839366Z","iopub.status.idle":"2024-07-21T05:06:44.859737Z","shell.execute_reply.started":"2024-07-21T05:06:44.839334Z","shell.execute_reply":"2024-07-21T05:06:44.858864Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\nout_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n\nfor index, word in enumerate(vocab):\n  if index == 0:\n    continue  # skip 0, it's padding.\n  vec = weights[index]\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n  out_m.write(word + \"\\n\")\nout_v.close()\nout_m.close()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:07:13.210484Z","iopub.execute_input":"2024-07-21T05:07:13.210867Z","iopub.status.idle":"2024-07-21T05:07:13.565830Z","shell.execute_reply.started":"2024-07-21T05:07:13.210829Z","shell.execute_reply":"2024-07-21T05:07:13.564998Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"try:\n  from google.colab import files\n  files.download('vectors.tsv')\n  files.download('metadata.tsv')\nexcept Exception:\n  pass","metadata":{"execution":{"iopub.status.busy":"2024-07-21T05:07:28.870112Z","iopub.execute_input":"2024-07-21T05:07:28.870542Z","iopub.status.idle":"2024-07-21T05:07:28.875821Z","shell.execute_reply.started":"2024-07-21T05:07:28.870513Z","shell.execute_reply":"2024-07-21T05:07:28.874816Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}