{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.tensorflow.org/text/tutorials/word2vec","metadata":{}},{"cell_type":"code","source":"import io\nimport re\nimport string\nimport tqdm\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:30.616348Z","iopub.execute_input":"2024-07-19T19:00:30.616786Z","iopub.status.idle":"2024-07-19T19:00:47.096466Z","shell.execute_reply.started":"2024-07-19T19:00:30.616754Z","shell.execute_reply":"2024-07-19T19:00:47.095082Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-19 19:00:33.037351: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-19 19:00:33.037537: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-19 19:00:33.217819: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the TensorBoard notebook extension\n%load_ext tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.099205Z","iopub.execute_input":"2024-07-19T19:00:47.100207Z","iopub.status.idle":"2024-07-19T19:00:47.127397Z","shell.execute_reply.started":"2024-07-19T19:00:47.100141Z","shell.execute_reply":"2024-07-19T19:00:47.125878Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.128970Z","iopub.execute_input":"2024-07-19T19:00:47.129416Z","iopub.status.idle":"2024-07-19T19:00:47.135792Z","shell.execute_reply.started":"2024-07-19T19:00:47.129371Z","shell.execute_reply":"2024-07-19T19:00:47.134290Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sentence = \"The wide road shimmered in the hot sun\"\ntokens = list(sentence.lower().split())\nprint(len(tokens))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.138696Z","iopub.execute_input":"2024-07-19T19:00:47.139111Z","iopub.status.idle":"2024-07-19T19:00:47.149788Z","shell.execute_reply.started":"2024-07-19T19:00:47.139081Z","shell.execute_reply":"2024-07-19T19:00:47.148459Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"8\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab, index = {}, 1  # start indexing from 1\nvocab['<pad>'] = 0  # add a padding token\nfor token in tokens:\n  if token not in vocab:\n    vocab[token] = index\n    index += 1\nvocab_size = len(vocab)\nprint(vocab)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.151273Z","iopub.execute_input":"2024-07-19T19:00:47.151634Z","iopub.status.idle":"2024-07-19T19:00:47.166108Z","shell.execute_reply.started":"2024-07-19T19:00:47.151604Z","shell.execute_reply":"2024-07-19T19:00:47.164531Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n","output_type":"stream"}]},{"cell_type":"code","source":"inverse_vocab = {index: token for token, index in vocab.items()}\nprint(inverse_vocab)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.167623Z","iopub.execute_input":"2024-07-19T19:00:47.168006Z","iopub.status.idle":"2024-07-19T19:00:47.179685Z","shell.execute_reply.started":"2024-07-19T19:00:47.167975Z","shell.execute_reply":"2024-07-19T19:00:47.178102Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n","output_type":"stream"}]},{"cell_type":"code","source":"example_sequence = [vocab[word] for word in tokens]\nprint(example_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.181468Z","iopub.execute_input":"2024-07-19T19:00:47.181860Z","iopub.status.idle":"2024-07-19T19:00:47.195056Z","shell.execute_reply.started":"2024-07-19T19:00:47.181815Z","shell.execute_reply":"2024-07-19T19:00:47.193562Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[1, 2, 3, 4, 5, 1, 6, 7]\n","output_type":"stream"}]},{"cell_type":"code","source":"window_size = 2\npositive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n      example_sequence,\n      vocabulary_size=vocab_size,\n      window_size=window_size,\n      negative_samples=0)\nprint(len(positive_skip_grams))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.196945Z","iopub.execute_input":"2024-07-19T19:00:47.197396Z","iopub.status.idle":"2024-07-19T19:00:47.210002Z","shell.execute_reply.started":"2024-07-19T19:00:47.197365Z","shell.execute_reply":"2024-07-19T19:00:47.208221Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"26\n","output_type":"stream"}]},{"cell_type":"code","source":"for target, context in positive_skip_grams[:5]:\n  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.212049Z","iopub.execute_input":"2024-07-19T19:00:47.212679Z","iopub.status.idle":"2024-07-19T19:00:47.224229Z","shell.execute_reply.started":"2024-07-19T19:00:47.212624Z","shell.execute_reply":"2024-07-19T19:00:47.222595Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(2, 4): (wide, shimmered)\n(4, 1): (shimmered, the)\n(5, 3): (in, road)\n(3, 1): (road, the)\n(5, 1): (in, the)\n","output_type":"stream"}]},{"cell_type":"code","source":"target_word, context_word = positive_skip_grams[0]\n\n# Set the number of negative samples per positive context.\nnum_ns = 4\n\ncontext_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\nnegative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n    true_classes=context_class,  # class that should be sampled as 'positive'\n    num_true=1,  # each positive skip-gram has 1 positive context class\n    num_sampled=num_ns,  # number of negative context words to sample\n    unique=True,  # all the negative samples should be unique\n    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n    seed=SEED,  # seed for reproducibility\n    name=\"negative_sampling\"  # name of this operation\n)\nprint(negative_sampling_candidates)\nprint([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.228112Z","iopub.execute_input":"2024-07-19T19:00:47.228691Z","iopub.status.idle":"2024-07-19T19:00:47.283924Z","shell.execute_reply.started":"2024-07-19T19:00:47.228659Z","shell.execute_reply":"2024-07-19T19:00:47.282022Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n['wide', 'the', 'shimmered', 'road']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reduce a dimension so you can use concatenation (in the next step).\nsqueezed_context_class = tf.squeeze(context_class, 1)\n\n# Concatenate a positive context word with negative sampled words.\ncontext = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n\n# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\nlabel = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\ntarget = target_word","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.286153Z","iopub.execute_input":"2024-07-19T19:00:47.286682Z","iopub.status.idle":"2024-07-19T19:00:47.296995Z","shell.execute_reply.started":"2024-07-19T19:00:47.286643Z","shell.execute_reply":"2024-07-19T19:00:47.295800Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(f\"target_index    : {target}\")\nprint(f\"target_word     : {inverse_vocab[target_word]}\")\nprint(f\"context_indices : {context}\")\nprint(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\nprint(f\"label           : {label}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.298659Z","iopub.execute_input":"2024-07-19T19:00:47.299063Z","iopub.status.idle":"2024-07-19T19:00:47.313512Z","shell.execute_reply.started":"2024-07-19T19:00:47.299031Z","shell.execute_reply":"2024-07-19T19:00:47.312102Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"target_index    : 2\ntarget_word     : wide\ncontext_indices : [4 2 1 4 3]\ncontext_words   : ['shimmered', 'wide', 'the', 'shimmered', 'road']\nlabel           : [1 0 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"target  :\", target)\nprint(\"context :\", context)\nprint(\"label   :\", label)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.315487Z","iopub.execute_input":"2024-07-19T19:00:47.315974Z","iopub.status.idle":"2024-07-19T19:00:47.324860Z","shell.execute_reply.started":"2024-07-19T19:00:47.315934Z","shell.execute_reply":"2024-07-19T19:00:47.323634Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"target  : 2\ncontext : tf.Tensor([4 2 1 4 3], shape=(5,), dtype=int64)\nlabel   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\nprint(sampling_table)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.326790Z","iopub.execute_input":"2024-07-19T19:00:47.327379Z","iopub.status.idle":"2024-07-19T19:00:47.340753Z","shell.execute_reply.started":"2024-07-19T19:00:47.327335Z","shell.execute_reply":"2024-07-19T19:00:47.339565Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n 0.01212381 0.01347162 0.01474487 0.0159558 ]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generates skip-gram pairs with negative sampling for a list of sequences\n# (int-encoded sentences) based on window size, number of negative samples\n# and vocabulary size.\ndef generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n  # Elements of each training example are appended to these lists.\n  targets, contexts, labels = [], [], []\n\n  # Build the sampling table for `vocab_size` tokens.\n  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n\n  # Iterate over all sequences (sentences) in the dataset.\n  for sequence in tqdm.tqdm(sequences):\n\n    # Generate positive skip-gram pairs for a sequence (sentence).\n    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n          sequence,\n          vocabulary_size=vocab_size,\n          sampling_table=sampling_table,\n          window_size=window_size,\n          negative_samples=0)\n    \n      # Iterate over each positive skip-gram pair to produce training examples\n    # with a positive context word and negative samples.\n    for target_word, context_word in positive_skip_grams:\n      context_class = tf.expand_dims(\n          tf.constant([context_word], dtype=\"int64\"), 1)\n      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n          true_classes=context_class,\n          num_true=1,\n          num_sampled=num_ns,\n          unique=True,\n          range_max=vocab_size,\n          seed=seed,\n          name=\"negative_sampling\")\n\n      # Build context and label vectors (for one target word)\n      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n\n      # Append each element from the training example to global lists.\n      targets.append(target_word)\n      contexts.append(context)\n      labels.append(label)\n\n  return targets, contexts, labels","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:04:04.697012Z","iopub.execute_input":"2024-07-19T19:04:04.697468Z","iopub.status.idle":"2024-07-19T19:04:04.710513Z","shell.execute_reply.started":"2024-07-19T19:04:04.697437Z","shell.execute_reply":"2024-07-19T19:04:04.709229Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Now to try an entire dataset (corpus)","metadata":{}},{"cell_type":"code","source":"path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:05:31.183684Z","iopub.execute_input":"2024-07-19T19:05:31.184190Z","iopub.status.idle":"2024-07-19T19:05:31.412051Z","shell.execute_reply.started":"2024-07-19T19:05:31.184140Z","shell.execute_reply":"2024-07-19T19:05:31.410640Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(path_to_file) as f:\n  lines = f.read().splitlines()\nfor line in lines[:20]:\n  print(line)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:06:19.356452Z","iopub.execute_input":"2024-07-19T19:06:19.357776Z","iopub.status.idle":"2024-07-19T19:06:19.374673Z","shell.execute_reply.started":"2024-07-19T19:06:19.357729Z","shell.execute_reply":"2024-07-19T19:06:19.373369Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\n","output_type":"stream"}]},{"cell_type":"code","source":"text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:07:20.983659Z","iopub.execute_input":"2024-07-19T19:07:20.984153Z","iopub.status.idle":"2024-07-19T19:07:21.064801Z","shell.execute_reply.started":"2024-07-19T19:07:20.984117Z","shell.execute_reply":"2024-07-19T19:07:21.063470Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Now, create a custom standardization function to lowercase the text and\n# remove punctuation.\ndef custom_standardization(input_data):\n  lowercase = tf.strings.lower(input_data)\n  return tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation), '')\n\n\n# Define the vocabulary size and the number of words in a sequence.\nvocab_size = 4096\nsequence_length = 10\n\n# Use the `TextVectorization` layer to normalize, split, and map strings to\n# integers. Set the `output_sequence_length` length to pad all samples to the\n# same length.\nvectorize_layer = layers.TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size,\n    output_mode='int',\n    output_sequence_length=sequence_length)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:08:10.951419Z","iopub.execute_input":"2024-07-19T19:08:10.951890Z","iopub.status.idle":"2024-07-19T19:08:10.974356Z","shell.execute_reply.started":"2024-07-19T19:08:10.951855Z","shell.execute_reply":"2024-07-19T19:08:10.972878Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"vectorize_layer.adapt(text_ds.batch(1024))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:08:25.257563Z","iopub.execute_input":"2024-07-19T19:08:25.258008Z","iopub.status.idle":"2024-07-19T19:08:26.991069Z","shell.execute_reply.started":"2024-07-19T19:08:25.257974Z","shell.execute_reply":"2024-07-19T19:08:26.989716Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Save the created vocabulary for reference.\ninverse_vocab = vectorize_layer.get_vocabulary()\nprint(inverse_vocab[:20])","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:08:40.085168Z","iopub.execute_input":"2024-07-19T19:08:40.085602Z","iopub.status.idle":"2024-07-19T19:08:40.115380Z","shell.execute_reply.started":"2024-07-19T19:08:40.085573Z","shell.execute_reply":"2024-07-19T19:08:40.113789Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Vectorize the data in text_ds.\ntext_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:08:58.375740Z","iopub.execute_input":"2024-07-19T19:08:58.376717Z","iopub.status.idle":"2024-07-19T19:08:58.500358Z","shell.execute_reply.started":"2024-07-19T19:08:58.376680Z","shell.execute_reply":"2024-07-19T19:08:58.498812Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"sequences = list(text_vector_ds.as_numpy_iterator())\nprint(len(sequences))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:09:25.346460Z","iopub.execute_input":"2024-07-19T19:09:25.346868Z","iopub.status.idle":"2024-07-19T19:09:31.413521Z","shell.execute_reply.started":"2024-07-19T19:09:25.346837Z","shell.execute_reply":"2024-07-19T19:09:31.412285Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"32777\n","output_type":"stream"}]},{"cell_type":"code","source":"for seq in sequences[:5]:\n  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:09:39.008664Z","iopub.execute_input":"2024-07-19T19:09:39.009494Z","iopub.status.idle":"2024-07-19T19:09:39.017087Z","shell.execute_reply.started":"2024-07-19T19:09:39.009457Z","shell.execute_reply":"2024-07-19T19:09:39.015683Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}