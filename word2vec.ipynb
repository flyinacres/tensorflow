{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.tensorflow.org/text/tutorials/word2vec","metadata":{}},{"cell_type":"code","source":"import io\nimport re\nimport string\nimport tqdm\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:30.616348Z","iopub.execute_input":"2024-07-19T19:00:30.616786Z","iopub.status.idle":"2024-07-19T19:00:47.096466Z","shell.execute_reply.started":"2024-07-19T19:00:30.616754Z","shell.execute_reply":"2024-07-19T19:00:47.095082Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-19 19:00:33.037351: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-19 19:00:33.037537: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-19 19:00:33.217819: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the TensorBoard notebook extension\n%load_ext tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.099205Z","iopub.execute_input":"2024-07-19T19:00:47.100207Z","iopub.status.idle":"2024-07-19T19:00:47.127397Z","shell.execute_reply.started":"2024-07-19T19:00:47.100141Z","shell.execute_reply":"2024-07-19T19:00:47.125878Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.128970Z","iopub.execute_input":"2024-07-19T19:00:47.129416Z","iopub.status.idle":"2024-07-19T19:00:47.135792Z","shell.execute_reply.started":"2024-07-19T19:00:47.129371Z","shell.execute_reply":"2024-07-19T19:00:47.134290Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sentence = \"The wide road shimmered in the hot sun\"\ntokens = list(sentence.lower().split())\nprint(len(tokens))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.138696Z","iopub.execute_input":"2024-07-19T19:00:47.139111Z","iopub.status.idle":"2024-07-19T19:00:47.149788Z","shell.execute_reply.started":"2024-07-19T19:00:47.139081Z","shell.execute_reply":"2024-07-19T19:00:47.148459Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"8\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab, index = {}, 1  # start indexing from 1\nvocab['<pad>'] = 0  # add a padding token\nfor token in tokens:\n  if token not in vocab:\n    vocab[token] = index\n    index += 1\nvocab_size = len(vocab)\nprint(vocab)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.151273Z","iopub.execute_input":"2024-07-19T19:00:47.151634Z","iopub.status.idle":"2024-07-19T19:00:47.166108Z","shell.execute_reply.started":"2024-07-19T19:00:47.151604Z","shell.execute_reply":"2024-07-19T19:00:47.164531Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n","output_type":"stream"}]},{"cell_type":"code","source":"inverse_vocab = {index: token for token, index in vocab.items()}\nprint(inverse_vocab)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.167623Z","iopub.execute_input":"2024-07-19T19:00:47.168006Z","iopub.status.idle":"2024-07-19T19:00:47.179685Z","shell.execute_reply.started":"2024-07-19T19:00:47.167975Z","shell.execute_reply":"2024-07-19T19:00:47.178102Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n","output_type":"stream"}]},{"cell_type":"code","source":"example_sequence = [vocab[word] for word in tokens]\nprint(example_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.181468Z","iopub.execute_input":"2024-07-19T19:00:47.181860Z","iopub.status.idle":"2024-07-19T19:00:47.195056Z","shell.execute_reply.started":"2024-07-19T19:00:47.181815Z","shell.execute_reply":"2024-07-19T19:00:47.193562Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[1, 2, 3, 4, 5, 1, 6, 7]\n","output_type":"stream"}]},{"cell_type":"code","source":"window_size = 2\npositive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n      example_sequence,\n      vocabulary_size=vocab_size,\n      window_size=window_size,\n      negative_samples=0)\nprint(len(positive_skip_grams))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.196945Z","iopub.execute_input":"2024-07-19T19:00:47.197396Z","iopub.status.idle":"2024-07-19T19:00:47.210002Z","shell.execute_reply.started":"2024-07-19T19:00:47.197365Z","shell.execute_reply":"2024-07-19T19:00:47.208221Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"26\n","output_type":"stream"}]},{"cell_type":"code","source":"for target, context in positive_skip_grams[:5]:\n  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.212049Z","iopub.execute_input":"2024-07-19T19:00:47.212679Z","iopub.status.idle":"2024-07-19T19:00:47.224229Z","shell.execute_reply.started":"2024-07-19T19:00:47.212624Z","shell.execute_reply":"2024-07-19T19:00:47.222595Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(2, 4): (wide, shimmered)\n(4, 1): (shimmered, the)\n(5, 3): (in, road)\n(3, 1): (road, the)\n(5, 1): (in, the)\n","output_type":"stream"}]},{"cell_type":"code","source":"target_word, context_word = positive_skip_grams[0]\n\n# Set the number of negative samples per positive context.\nnum_ns = 4\n\ncontext_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\nnegative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n    true_classes=context_class,  # class that should be sampled as 'positive'\n    num_true=1,  # each positive skip-gram has 1 positive context class\n    num_sampled=num_ns,  # number of negative context words to sample\n    unique=True,  # all the negative samples should be unique\n    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n    seed=SEED,  # seed for reproducibility\n    name=\"negative_sampling\"  # name of this operation\n)\nprint(negative_sampling_candidates)\nprint([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.228112Z","iopub.execute_input":"2024-07-19T19:00:47.228691Z","iopub.status.idle":"2024-07-19T19:00:47.283924Z","shell.execute_reply.started":"2024-07-19T19:00:47.228659Z","shell.execute_reply":"2024-07-19T19:00:47.282022Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n['wide', 'the', 'shimmered', 'road']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reduce a dimension so you can use concatenation (in the next step).\nsqueezed_context_class = tf.squeeze(context_class, 1)\n\n# Concatenate a positive context word with negative sampled words.\ncontext = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n\n# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\nlabel = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\ntarget = target_word","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.286153Z","iopub.execute_input":"2024-07-19T19:00:47.286682Z","iopub.status.idle":"2024-07-19T19:00:47.296995Z","shell.execute_reply.started":"2024-07-19T19:00:47.286643Z","shell.execute_reply":"2024-07-19T19:00:47.295800Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(f\"target_index    : {target}\")\nprint(f\"target_word     : {inverse_vocab[target_word]}\")\nprint(f\"context_indices : {context}\")\nprint(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\nprint(f\"label           : {label}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.298659Z","iopub.execute_input":"2024-07-19T19:00:47.299063Z","iopub.status.idle":"2024-07-19T19:00:47.313512Z","shell.execute_reply.started":"2024-07-19T19:00:47.299031Z","shell.execute_reply":"2024-07-19T19:00:47.312102Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"target_index    : 2\ntarget_word     : wide\ncontext_indices : [4 2 1 4 3]\ncontext_words   : ['shimmered', 'wide', 'the', 'shimmered', 'road']\nlabel           : [1 0 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"target  :\", target)\nprint(\"context :\", context)\nprint(\"label   :\", label)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.315487Z","iopub.execute_input":"2024-07-19T19:00:47.315974Z","iopub.status.idle":"2024-07-19T19:00:47.324860Z","shell.execute_reply.started":"2024-07-19T19:00:47.315934Z","shell.execute_reply":"2024-07-19T19:00:47.323634Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"target  : 2\ncontext : tf.Tensor([4 2 1 4 3], shape=(5,), dtype=int64)\nlabel   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\nprint(sampling_table)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:00:47.326790Z","iopub.execute_input":"2024-07-19T19:00:47.327379Z","iopub.status.idle":"2024-07-19T19:00:47.340753Z","shell.execute_reply.started":"2024-07-19T19:00:47.327335Z","shell.execute_reply":"2024-07-19T19:00:47.339565Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n 0.01212381 0.01347162 0.01474487 0.0159558 ]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generates skip-gram pairs with negative sampling for a list of sequences\n# (int-encoded sentences) based on window size, number of negative samples\n# and vocabulary size.\ndef generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n  # Elements of each training example are appended to these lists.\n  targets, contexts, labels = [], [], []\n\n  # Build the sampling table for `vocab_size` tokens.\n  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n\n  # Iterate over all sequences (sentences) in the dataset.\n  for sequence in tqdm.tqdm(sequences):\n\n    # Generate positive skip-gram pairs for a sequence (sentence).\n    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n          sequence,\n          vocabulary_size=vocab_size,\n          sampling_table=sampling_table,\n          window_size=window_size,\n          negative_samples=0)\n    \n      # Iterate over each positive skip-gram pair to produce training examples\n    # with a positive context word and negative samples.\n    for target_word, context_word in positive_skip_grams:\n      context_class = tf.expand_dims(\n          tf.constant([context_word], dtype=\"int64\"), 1)\n      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n          true_classes=context_class,\n          num_true=1,\n          num_sampled=num_ns,\n          unique=True,\n          range_max=vocab_size,\n          seed=seed,\n          name=\"negative_sampling\")\n\n      # Build context and label vectors (for one target word)\n      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n\n      # Append each element from the training example to global lists.\n      targets.append(target_word)\n      contexts.append(context)\n      labels.append(label)\n\n  return targets, contexts, labels","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:04:04.697012Z","iopub.execute_input":"2024-07-19T19:04:04.697468Z","iopub.status.idle":"2024-07-19T19:04:04.710513Z","shell.execute_reply.started":"2024-07-19T19:04:04.697437Z","shell.execute_reply":"2024-07-19T19:04:04.709229Z"},"trusted":true},"execution_count":16,"outputs":[]}]}